{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c480d74a-d567-46ac-812c-a790df98caa7",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Network\n",
    "\n",
    "This file is used to run the optimally-determined model as per grid search in FFNN_Grid_Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7599a31-d302-46c6-a558-35b7d3a2ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3839974-e82f-43f6-9445-7da6e263123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikeras in c:\\users\\kirin\\miniconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: keras in c:\\users\\kirin\\miniconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: packaging>=0.21 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikeras) (23.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikeras) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem<0.32,>=0.23.1 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikeras) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikeras keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd374a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kirin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf9798b-aa96-4398-a317-e1cbad5518ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(first_layer=128, activation_1='relu', second_layer=64, activation_2='relu', activation_out='softmax', learn_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(first_layer, input_shape=(100,), activation=activation_1))\n",
    "    model.add(Dense(second_layer, activation=activation_2))\n",
    "    model.add(Dense(num_classes, activation=activation_out))\n",
    "    model.compile(optimizer=Adam(learning_rate=learn_rate), loss=LOSS_TYPE, metrics=METRICS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88ba10ed-ad73-40d5-b898-be5dab20bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('features.csv')\n",
    "num_classes = 120\n",
    "\n",
    "ACTIVATION_1 = 'relu'\n",
    "ACTIVATION_2 = 'relu'\n",
    "ACTIVATION_OUT = 'softmax'\n",
    "TEST_SIZE = 0.25\n",
    "EPOCHS = 20\n",
    "METRICS = ['accuracy']\n",
    "LOSS_TYPE = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84336ead-25d3-401b-9e0e-311c35f29391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "327/327 [==============================] - 1s 2ms/step - loss: 2.8517 - accuracy: 0.3802 - val_loss: 1.2323 - val_accuracy: 0.6711\n",
      "Epoch 2/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.9036 - accuracy: 0.7442 - val_loss: 0.8698 - val_accuracy: 0.7452\n",
      "Epoch 3/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.6753 - accuracy: 0.7946 - val_loss: 0.7893 - val_accuracy: 0.7614\n",
      "Epoch 4/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.5798 - accuracy: 0.8218 - val_loss: 0.7593 - val_accuracy: 0.7653\n",
      "Epoch 5/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.5219 - accuracy: 0.8348 - val_loss: 0.7318 - val_accuracy: 0.7770\n",
      "Epoch 6/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.4755 - accuracy: 0.8468 - val_loss: 0.7182 - val_accuracy: 0.7876\n",
      "Epoch 7/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.4407 - accuracy: 0.8585 - val_loss: 0.7303 - val_accuracy: 0.7795\n",
      "Epoch 8/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.4123 - accuracy: 0.8644 - val_loss: 0.7297 - val_accuracy: 0.7856\n",
      "Epoch 9/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.3889 - accuracy: 0.8756 - val_loss: 0.7257 - val_accuracy: 0.7834\n",
      "Epoch 10/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.3647 - accuracy: 0.8842 - val_loss: 0.7278 - val_accuracy: 0.7890\n",
      "Epoch 11/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.3445 - accuracy: 0.8889 - val_loss: 0.7397 - val_accuracy: 0.7859\n",
      "Epoch 12/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.3262 - accuracy: 0.8955 - val_loss: 0.7459 - val_accuracy: 0.7878\n",
      "Epoch 13/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.3079 - accuracy: 0.9042 - val_loss: 0.7588 - val_accuracy: 0.7868\n",
      "Epoch 14/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.2921 - accuracy: 0.9060 - val_loss: 0.7637 - val_accuracy: 0.7881\n",
      "Epoch 15/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.2764 - accuracy: 0.9137 - val_loss: 0.7708 - val_accuracy: 0.7876\n",
      "Epoch 16/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.2603 - accuracy: 0.9173 - val_loss: 0.7785 - val_accuracy: 0.7859\n",
      "Epoch 17/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.2469 - accuracy: 0.9228 - val_loss: 0.7910 - val_accuracy: 0.7861\n",
      "Epoch 18/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.2346 - accuracy: 0.9260 - val_loss: 0.7944 - val_accuracy: 0.7890\n",
      "Epoch 19/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.2231 - accuracy: 0.9282 - val_loss: 0.8187 - val_accuracy: 0.7837\n",
      "Epoch 20/20\n",
      "327/327 [==============================] - 0s 1ms/step - loss: 0.2102 - accuracy: 0.9332 - val_loss: 0.8303 - val_accuracy: 0.7795\n",
      "128/128 [==============================] - 0s 614us/step\n",
      "128/128 [==============================] - 0s 685us/step - loss: 0.8303 - accuracy: 0.7795\n",
      "Test Accuracy: 0.7794909477233887\n",
      "Confusion Matrix for each label : \n",
      "[[[4056    3]\n",
      "  [   4   23]]\n",
      "\n",
      " [[4030    5]\n",
      "  [   2   49]]\n",
      "\n",
      " [[4037    6]\n",
      "  [   1   42]]\n",
      "\n",
      " [[4027    8]\n",
      "  [  10   41]]\n",
      "\n",
      " [[4024   28]\n",
      "  [  11   23]]\n",
      "\n",
      " [[4041   12]\n",
      "  [  13   20]]\n",
      "\n",
      " [[4036   10]\n",
      "  [   8   32]]\n",
      "\n",
      " [[4025    7]\n",
      "  [   4   50]]\n",
      "\n",
      " [[4041   12]\n",
      "  [   5   28]]\n",
      "\n",
      " [[4043    4]\n",
      "  [  15   24]]\n",
      "\n",
      " [[4048    2]\n",
      "  [   3   33]]\n",
      "\n",
      " [[4040    4]\n",
      "  [   3   39]]\n",
      "\n",
      " [[4054    0]\n",
      "  [   3   29]]\n",
      "\n",
      " [[4040    5]\n",
      "  [   7   34]]\n",
      "\n",
      " [[4060    1]\n",
      "  [  10   15]]\n",
      "\n",
      " [[4037   13]\n",
      "  [  12   24]]\n",
      "\n",
      " [[4049    5]\n",
      "  [   4   28]]\n",
      "\n",
      " [[4041    8]\n",
      "  [   6   31]]\n",
      "\n",
      " [[4045    8]\n",
      "  [   4   29]]\n",
      "\n",
      " [[4050   12]\n",
      "  [   8   16]]\n",
      "\n",
      " [[4055    7]\n",
      "  [   2   22]]\n",
      "\n",
      " [[4061    2]\n",
      "  [   3   20]]\n",
      "\n",
      " [[4044    9]\n",
      "  [  12   21]]\n",
      "\n",
      " [[4030   14]\n",
      "  [   4   38]]\n",
      "\n",
      " [[4055    2]\n",
      "  [   4   25]]\n",
      "\n",
      " [[4040    6]\n",
      "  [   6   34]]\n",
      "\n",
      " [[4050    5]\n",
      "  [  10   21]]\n",
      "\n",
      " [[4046    3]\n",
      "  [   8   29]]\n",
      "\n",
      " [[4058    8]\n",
      "  [   6   14]]\n",
      "\n",
      " [[4047    3]\n",
      "  [   2   34]]\n",
      "\n",
      " [[4057    3]\n",
      "  [   1   25]]\n",
      "\n",
      " [[4050    8]\n",
      "  [   6   22]]\n",
      "\n",
      " [[4041   10]\n",
      "  [   7   28]]\n",
      "\n",
      " [[4055    3]\n",
      "  [   8   20]]\n",
      "\n",
      " [[4034   16]\n",
      "  [  18   18]]\n",
      "\n",
      " [[4050    5]\n",
      "  [   7   24]]\n",
      "\n",
      " [[4050    2]\n",
      "  [   6   28]]\n",
      "\n",
      " [[4043   13]\n",
      "  [   6   24]]\n",
      "\n",
      " [[4039   15]\n",
      "  [   5   27]]\n",
      "\n",
      " [[4049    7]\n",
      "  [  17   13]]\n",
      "\n",
      " [[4032    7]\n",
      "  [  12   35]]\n",
      "\n",
      " [[4052    2]\n",
      "  [   9   23]]\n",
      "\n",
      " [[4040    8]\n",
      "  [  11   27]]\n",
      "\n",
      " [[4048    8]\n",
      "  [  22    8]]\n",
      "\n",
      " [[4056    4]\n",
      "  [  10   16]]\n",
      "\n",
      " [[4047    7]\n",
      "  [   3   29]]\n",
      "\n",
      " [[4044    7]\n",
      "  [  12   23]]\n",
      "\n",
      " [[4053    7]\n",
      "  [   1   25]]\n",
      "\n",
      " [[4062    3]\n",
      "  [   1   20]]\n",
      "\n",
      " [[4035   14]\n",
      "  [  12   25]]\n",
      "\n",
      " [[4032   11]\n",
      "  [  12   31]]\n",
      "\n",
      " [[4035   13]\n",
      "  [   6   32]]\n",
      "\n",
      " [[4054    3]\n",
      "  [   6   23]]\n",
      "\n",
      " [[4049    9]\n",
      "  [   8   20]]\n",
      "\n",
      " [[4052    5]\n",
      "  [   3   26]]\n",
      "\n",
      " [[4044    3]\n",
      "  [   3   36]]\n",
      "\n",
      " [[4043    3]\n",
      "  [   8   32]]\n",
      "\n",
      " [[4047   12]\n",
      "  [   4   23]]\n",
      "\n",
      " [[4058    4]\n",
      "  [   5   19]]\n",
      "\n",
      " [[4034   10]\n",
      "  [  23   19]]\n",
      "\n",
      " [[4035   15]\n",
      "  [   5   31]]\n",
      "\n",
      " [[4036    7]\n",
      "  [   3   40]]\n",
      "\n",
      " [[4050    3]\n",
      "  [   0   33]]\n",
      "\n",
      " [[4041   11]\n",
      "  [  10   24]]\n",
      "\n",
      " [[4058    1]\n",
      "  [  10   17]]\n",
      "\n",
      " [[4054    2]\n",
      "  [   1   29]]\n",
      "\n",
      " [[4048   11]\n",
      "  [   4   23]]\n",
      "\n",
      " [[4035   15]\n",
      "  [   9   27]]\n",
      "\n",
      " [[4036   13]\n",
      "  [  13   24]]\n",
      "\n",
      " [[4046    0]\n",
      "  [   3   37]]\n",
      "\n",
      " [[4039   11]\n",
      "  [  17   19]]\n",
      "\n",
      " [[4029   18]\n",
      "  [   5   34]]\n",
      "\n",
      " [[4059    1]\n",
      "  [   7   19]]\n",
      "\n",
      " [[4042    6]\n",
      "  [   2   36]]\n",
      "\n",
      " [[4044    3]\n",
      "  [   6   33]]\n",
      "\n",
      " [[4049    4]\n",
      "  [  13   20]]\n",
      "\n",
      " [[4028   22]\n",
      "  [  15   21]]\n",
      "\n",
      " [[4051    2]\n",
      "  [  18   15]]\n",
      "\n",
      " [[4030    9]\n",
      "  [   9   38]]\n",
      "\n",
      " [[4033   11]\n",
      "  [  15   27]]\n",
      "\n",
      " [[4041    0]\n",
      "  [   8   37]]\n",
      "\n",
      " [[4043   13]\n",
      "  [   7   23]]\n",
      "\n",
      " [[4048    5]\n",
      "  [   1   32]]\n",
      "\n",
      " [[4044    5]\n",
      "  [  14   23]]\n",
      "\n",
      " [[4041    3]\n",
      "  [   5   37]]\n",
      "\n",
      " [[4054    2]\n",
      "  [   5   25]]\n",
      "\n",
      " [[4041    6]\n",
      "  [   4   35]]\n",
      "\n",
      " [[4055    1]\n",
      "  [   2   28]]\n",
      "\n",
      " [[4043    4]\n",
      "  [   6   33]]\n",
      "\n",
      " [[4044   10]\n",
      "  [   7   25]]\n",
      "\n",
      " [[4053    4]\n",
      "  [  11   18]]\n",
      "\n",
      " [[4056    3]\n",
      "  [   4   23]]\n",
      "\n",
      " [[4054    1]\n",
      "  [   0   31]]\n",
      "\n",
      " [[4038    5]\n",
      "  [   6   37]]\n",
      "\n",
      " [[4050    6]\n",
      "  [   3   27]]\n",
      "\n",
      " [[4045    7]\n",
      "  [   5   29]]\n",
      "\n",
      " [[4044    5]\n",
      "  [   6   31]]\n",
      "\n",
      " [[4012   16]\n",
      "  [   9   49]]\n",
      "\n",
      " [[4038    9]\n",
      "  [   3   36]]\n",
      "\n",
      " [[4057    5]\n",
      "  [   7   17]]\n",
      "\n",
      " [[4028   19]\n",
      "  [  14   25]]\n",
      "\n",
      " [[4036   13]\n",
      "  [  14   23]]\n",
      "\n",
      " [[4039    8]\n",
      "  [  23   16]]\n",
      "\n",
      " [[4035   20]\n",
      "  [   8   23]]\n",
      "\n",
      " [[4031   23]\n",
      "  [   5   27]]\n",
      "\n",
      " [[4054    2]\n",
      "  [   2   28]]\n",
      "\n",
      " [[4046    5]\n",
      "  [   7   28]]\n",
      "\n",
      " [[4050    4]\n",
      "  [   7   25]]\n",
      "\n",
      " [[4027   14]\n",
      "  [  10   35]]\n",
      "\n",
      " [[4054    5]\n",
      "  [  15   12]]\n",
      "\n",
      " [[4044    5]\n",
      "  [   8   29]]\n",
      "\n",
      " [[4036   10]\n",
      "  [  13   27]]\n",
      "\n",
      " [[4048   10]\n",
      "  [   5   23]]\n",
      "\n",
      " [[4040   20]\n",
      "  [   6   20]]\n",
      "\n",
      " [[4059    5]\n",
      "  [   1   21]]\n",
      "\n",
      " [[4051    4]\n",
      "  [   6   25]]\n",
      "\n",
      " [[4043    5]\n",
      "  [   9   29]]\n",
      "\n",
      " [[4041    8]\n",
      "  [  17   20]]\n",
      "\n",
      " [[4046   12]\n",
      "  [  11   17]]]\n",
      "Classification Report : \n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                 affenpinscher       0.88      0.85      0.87        27\n",
      "                  afghan_hound       0.91      0.96      0.93        51\n",
      "           african_hunting_dog       0.88      0.98      0.92        43\n",
      "                      airedale       0.84      0.80      0.82        51\n",
      "american_staffordshire_terrier       0.45      0.68      0.54        34\n",
      "                   appenzeller       0.62      0.61      0.62        33\n",
      "            australian_terrier       0.76      0.80      0.78        40\n",
      "                       basenji       0.88      0.93      0.90        54\n",
      "                        basset       0.70      0.85      0.77        33\n",
      "                        beagle       0.86      0.62      0.72        39\n",
      "            bedlington_terrier       0.94      0.92      0.93        36\n",
      "          bernese_mountain_dog       0.91      0.93      0.92        42\n",
      "              blenheim_spaniel       1.00      0.91      0.95        32\n",
      "                    bloodhound       0.87      0.83      0.85        41\n",
      "                      bluetick       0.94      0.60      0.73        25\n",
      "                 border_collie       0.65      0.67      0.66        36\n",
      "                border_terrier       0.85      0.88      0.86        32\n",
      "                        borzoi       0.79      0.84      0.82        37\n",
      "                   boston_bull       0.78      0.88      0.83        33\n",
      "          bouvier_des_flandres       0.57      0.67      0.62        24\n",
      "                         boxer       0.76      0.92      0.83        24\n",
      "             brabancon_griffon       0.91      0.87      0.89        23\n",
      "                        briard       0.70      0.64      0.67        33\n",
      "              brittany_spaniel       0.73      0.90      0.81        42\n",
      "                  bull_mastiff       0.93      0.86      0.89        29\n",
      "                         cairn       0.85      0.85      0.85        40\n",
      "                      cardigan       0.81      0.68      0.74        31\n",
      "      chesapeake_bay_retriever       0.91      0.78      0.84        37\n",
      "                     chihuahua       0.64      0.70      0.67        20\n",
      "                          chow       0.92      0.94      0.93        36\n",
      "                       clumber       0.89      0.96      0.93        26\n",
      "              coated_retriever       0.73      0.79      0.76        28\n",
      "        coated_wheaten_terrier       0.74      0.80      0.77        35\n",
      "                cocker_spaniel       0.87      0.71      0.78        28\n",
      "                        collie       0.53      0.50      0.51        36\n",
      "                dandie_dinmont       0.83      0.77      0.80        31\n",
      "                         dhole       0.93      0.82      0.88        34\n",
      "                         dingo       0.65      0.80      0.72        30\n",
      "                      doberman       0.64      0.84      0.73        32\n",
      "              english_foxhound       0.65      0.43      0.52        30\n",
      "                english_setter       0.83      0.74      0.79        47\n",
      "              english_springer       0.92      0.72      0.81        32\n",
      "                   entlebucher       0.77      0.71      0.74        38\n",
      "                    eskimo_dog       0.50      0.27      0.35        30\n",
      "                french_bulldog       0.80      0.62      0.70        26\n",
      "               german_shepherd       0.81      0.91      0.85        32\n",
      "               giant_schnauzer       0.77      0.66      0.71        35\n",
      "              golden_retriever       0.78      0.96      0.86        26\n",
      "                 gordon_setter       0.87      0.95      0.91        21\n",
      "                    great_dane       0.64      0.68      0.66        37\n",
      "                great_pyrenees       0.74      0.72      0.73        43\n",
      "    greater_swiss_mountain_dog       0.71      0.84      0.77        38\n",
      "                   groenendael       0.88      0.79      0.84        29\n",
      "            haired_fox_terrier       0.69      0.71      0.70        28\n",
      "                haired_pointer       0.84      0.90      0.87        29\n",
      "                  ibizan_hound       0.92      0.92      0.92        39\n",
      "                  irish_setter       0.91      0.80      0.85        40\n",
      "                 irish_terrier       0.66      0.85      0.74        27\n",
      "           irish_water_spaniel       0.83      0.79      0.81        24\n",
      "               irish_wolfhound       0.66      0.45      0.54        42\n",
      "             italian_greyhound       0.67      0.86      0.76        36\n",
      "              japanese_spaniel       0.85      0.93      0.89        43\n",
      "                      keeshond       0.92      1.00      0.96        33\n",
      "                        kelpie       0.69      0.71      0.70        34\n",
      "            kerry_blue_terrier       0.94      0.63      0.76        27\n",
      "                      komondor       0.94      0.97      0.95        30\n",
      "                        kuvasz       0.68      0.85      0.75        27\n",
      "            labrador_retriever       0.64      0.75      0.69        36\n",
      "              lakeland_terrier       0.65      0.65      0.65        37\n",
      "                      leonberg       1.00      0.93      0.96        40\n",
      "                         lhasa       0.63      0.53      0.58        36\n",
      "                      malamute       0.65      0.87      0.75        39\n",
      "                      malinois       0.95      0.73      0.83        26\n",
      "                   maltese_dog       0.86      0.95      0.90        38\n",
      "              mexican_hairless       0.92      0.85      0.88        39\n",
      "            miniature_pinscher       0.83      0.61      0.70        33\n",
      "              miniature_poodle       0.49      0.58      0.53        36\n",
      "           miniature_schnauzer       0.88      0.45      0.60        33\n",
      "                  newfoundland       0.81      0.81      0.81        47\n",
      "               norfolk_terrier       0.71      0.64      0.68        42\n",
      "            norwegian_elkhound       1.00      0.82      0.90        45\n",
      "               norwich_terrier       0.64      0.77      0.70        30\n",
      "          old_english_sheepdog       0.86      0.97      0.91        33\n",
      "                    otterhound       0.82      0.62      0.71        37\n",
      "                      papillon       0.93      0.88      0.90        42\n",
      "                      pekinese       0.93      0.83      0.88        30\n",
      "                      pembroke       0.85      0.90      0.88        39\n",
      "                    pomeranian       0.97      0.93      0.95        30\n",
      "                           pug       0.89      0.85      0.87        39\n",
      "                       redbone       0.71      0.78      0.75        32\n",
      "           rhodesian_ridgeback       0.82      0.62      0.71        29\n",
      "                    rottweiler       0.88      0.85      0.87        27\n",
      "                 saint_bernard       0.97      1.00      0.98        31\n",
      "                        saluki       0.88      0.86      0.87        43\n",
      "                       samoyed       0.82      0.90      0.86        30\n",
      "                    schipperke       0.81      0.85      0.83        34\n",
      "                scotch_terrier       0.86      0.84      0.85        37\n",
      "            scottish_deerhound       0.75      0.84      0.80        58\n",
      "              sealyham_terrier       0.80      0.92      0.86        39\n",
      "             shetland_sheepdog       0.77      0.71      0.74        24\n",
      "                siberian_husky       0.57      0.64      0.60        39\n",
      "                 silky_terrier       0.64      0.62      0.63        37\n",
      "     staffordshire_bullterrier       0.67      0.41      0.51        39\n",
      "               standard_poodle       0.53      0.74      0.62        31\n",
      "            standard_schnauzer       0.54      0.84      0.66        32\n",
      "                sussex_spaniel       0.93      0.93      0.93        30\n",
      "                 tan_coonhound       0.85      0.80      0.82        35\n",
      "               tibetan_mastiff       0.86      0.78      0.82        32\n",
      "               tibetan_terrier       0.71      0.78      0.74        45\n",
      "                    toy_poodle       0.71      0.44      0.55        27\n",
      "                   toy_terrier       0.85      0.78      0.82        37\n",
      "                           tzu       0.73      0.68      0.70        40\n",
      "                        vizsla       0.70      0.82      0.75        28\n",
      "                  walker_hound       0.50      0.77      0.61        26\n",
      "                    weimaraner       0.81      0.95      0.88        22\n",
      "        welsh_springer_spaniel       0.86      0.81      0.83        31\n",
      "   west_highland_white_terrier       0.85      0.76      0.81        38\n",
      "                       whippet       0.71      0.54      0.62        37\n",
      "             yorkshire_terrier       0.59      0.61      0.60        28\n",
      "\n",
      "                      accuracy                           0.78      4086\n",
      "                     macro avg       0.79      0.78      0.78      4086\n",
      "                  weighted avg       0.79      0.78      0.78      4086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = df.drop(columns=['Breed'])\n",
    "y = df['Breed']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n",
    "\n",
    "# Encode the categorical target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert the encoded labels to one-hot encoding\n",
    "y_train = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n",
    "optimal = create_model(first_layer=128, activation_1='relu', second_layer=64, activation_2='relu', activation_out='softmax', learn_rate=0.0005)\n",
    "optimal.fit(x_train, y_train, epochs=EPOCHS, batch_size=50, validation_data=(x_test, y_test))\n",
    "\n",
    "# Make predictions\n",
    "y_pred_opt = optimal.predict(x_test)\n",
    "\n",
    "# Convert continuous predictions to class labels\n",
    "y_pred_classes = np.argmax(y_pred_opt, axis=1)\n",
    "\n",
    "# Convert one-hot encoded y_test back to class labels\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Inverse transform the predicted and ground truth class labels to original breed names\n",
    "y_pred_breed = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_test_breed = label_encoder.inverse_transform(y_test_classes)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = optimal.evaluate(x_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "multi_conf_matrix = multilabel_confusion_matrix(y_test_breed, y_pred_breed)\n",
    "print(\"Confusion Matrix for each label : \")\n",
    "print(multi_conf_matrix)\n",
    "\n",
    "print(\"Classification Report : \")\n",
    "print(classification_report(y_test_breed, y_pred_breed))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d78e7-22ed-4931-a653-c07ae01c6117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
