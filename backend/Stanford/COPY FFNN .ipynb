{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c480d74a-d567-46ac-812c-a790df98caa7",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7599a31-d302-46c6-a558-35b7d3a2ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44bc7f72-631e-48c0-9b9d-909224a60244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikeras in c:\\users\\kirin\\miniconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: keras in c:\\users\\kirin\\miniconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: packaging>=0.21 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikeras) (23.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikeras) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem<0.32,>=0.23.1 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikeras) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikeras keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfd374a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "200cbb76-e40c-4f79-aef7-404b7e0777f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Breed</th>\n",
       "      <th>Feature 0</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature 90</th>\n",
       "      <th>Feature 91</th>\n",
       "      <th>Feature 92</th>\n",
       "      <th>Feature 93</th>\n",
       "      <th>Feature 94</th>\n",
       "      <th>Feature 95</th>\n",
       "      <th>Feature 96</th>\n",
       "      <th>Feature 97</th>\n",
       "      <th>Feature 98</th>\n",
       "      <th>Feature 99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brittany_spaniel</td>\n",
       "      <td>-5.279870</td>\n",
       "      <td>-2.208426</td>\n",
       "      <td>5.211814</td>\n",
       "      <td>-8.099501</td>\n",
       "      <td>-13.185220</td>\n",
       "      <td>-0.753768</td>\n",
       "      <td>-3.015471</td>\n",
       "      <td>0.825370</td>\n",
       "      <td>-4.269372</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.465695</td>\n",
       "      <td>-0.674558</td>\n",
       "      <td>0.123558</td>\n",
       "      <td>-1.240681</td>\n",
       "      <td>-1.069499</td>\n",
       "      <td>-0.714136</td>\n",
       "      <td>1.011624</td>\n",
       "      <td>-0.225715</td>\n",
       "      <td>-0.220330</td>\n",
       "      <td>1.140998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brittany_spaniel</td>\n",
       "      <td>0.524020</td>\n",
       "      <td>-4.321692</td>\n",
       "      <td>-5.808849</td>\n",
       "      <td>-0.097443</td>\n",
       "      <td>-13.157566</td>\n",
       "      <td>0.439865</td>\n",
       "      <td>-4.521323</td>\n",
       "      <td>-1.382092</td>\n",
       "      <td>-1.304059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.410429</td>\n",
       "      <td>0.396908</td>\n",
       "      <td>0.344853</td>\n",
       "      <td>0.243527</td>\n",
       "      <td>-0.416206</td>\n",
       "      <td>0.310021</td>\n",
       "      <td>0.025371</td>\n",
       "      <td>-0.221647</td>\n",
       "      <td>-0.306602</td>\n",
       "      <td>-0.855617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brittany_spaniel</td>\n",
       "      <td>4.484349</td>\n",
       "      <td>-11.409184</td>\n",
       "      <td>-2.061785</td>\n",
       "      <td>-8.795961</td>\n",
       "      <td>-10.736951</td>\n",
       "      <td>-0.650287</td>\n",
       "      <td>0.697348</td>\n",
       "      <td>8.464226</td>\n",
       "      <td>-2.961214</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.486026</td>\n",
       "      <td>0.636264</td>\n",
       "      <td>-0.315544</td>\n",
       "      <td>1.400241</td>\n",
       "      <td>-0.116869</td>\n",
       "      <td>-0.253275</td>\n",
       "      <td>0.918089</td>\n",
       "      <td>-0.645903</td>\n",
       "      <td>0.545067</td>\n",
       "      <td>-1.274210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brittany_spaniel</td>\n",
       "      <td>8.630311</td>\n",
       "      <td>-9.028896</td>\n",
       "      <td>-4.177602</td>\n",
       "      <td>-3.575223</td>\n",
       "      <td>-7.698362</td>\n",
       "      <td>-5.857273</td>\n",
       "      <td>-3.473359</td>\n",
       "      <td>6.915802</td>\n",
       "      <td>1.972978</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.718268</td>\n",
       "      <td>0.056995</td>\n",
       "      <td>0.152345</td>\n",
       "      <td>0.562226</td>\n",
       "      <td>0.279578</td>\n",
       "      <td>-0.277104</td>\n",
       "      <td>-0.956852</td>\n",
       "      <td>-0.531012</td>\n",
       "      <td>0.249064</td>\n",
       "      <td>-1.877944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brittany_spaniel</td>\n",
       "      <td>-6.459163</td>\n",
       "      <td>-5.178344</td>\n",
       "      <td>3.182314</td>\n",
       "      <td>-6.884826</td>\n",
       "      <td>-2.663270</td>\n",
       "      <td>-0.779802</td>\n",
       "      <td>-0.788943</td>\n",
       "      <td>4.521932</td>\n",
       "      <td>-3.636744</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.964177</td>\n",
       "      <td>0.368517</td>\n",
       "      <td>-0.274313</td>\n",
       "      <td>-0.534803</td>\n",
       "      <td>0.797063</td>\n",
       "      <td>-0.156896</td>\n",
       "      <td>1.027882</td>\n",
       "      <td>1.130965</td>\n",
       "      <td>0.696718</td>\n",
       "      <td>0.098893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Breed  Feature 0  Feature 1  Feature 2  Feature 3  Feature 4  \\\n",
       "0  brittany_spaniel  -5.279870  -2.208426   5.211814  -8.099501 -13.185220   \n",
       "1  brittany_spaniel   0.524020  -4.321692  -5.808849  -0.097443 -13.157566   \n",
       "2  brittany_spaniel   4.484349 -11.409184  -2.061785  -8.795961 -10.736951   \n",
       "3  brittany_spaniel   8.630311  -9.028896  -4.177602  -3.575223  -7.698362   \n",
       "4  brittany_spaniel  -6.459163  -5.178344   3.182314  -6.884826  -2.663270   \n",
       "\n",
       "   Feature 5  Feature 6  Feature 7  Feature 8  ...  Feature 90  Feature 91  \\\n",
       "0  -0.753768  -3.015471   0.825370  -4.269372  ...   -0.465695   -0.674558   \n",
       "1   0.439865  -4.521323  -1.382092  -1.304059  ...   -0.410429    0.396908   \n",
       "2  -0.650287   0.697348   8.464226  -2.961214  ...   -1.486026    0.636264   \n",
       "3  -5.857273  -3.473359   6.915802   1.972978  ...   -1.718268    0.056995   \n",
       "4  -0.779802  -0.788943   4.521932  -3.636744  ...   -0.964177    0.368517   \n",
       "\n",
       "   Feature 92  Feature 93  Feature 94  Feature 95  Feature 96  Feature 97  \\\n",
       "0    0.123558   -1.240681   -1.069499   -0.714136    1.011624   -0.225715   \n",
       "1    0.344853    0.243527   -0.416206    0.310021    0.025371   -0.221647   \n",
       "2   -0.315544    1.400241   -0.116869   -0.253275    0.918089   -0.645903   \n",
       "3    0.152345    0.562226    0.279578   -0.277104   -0.956852   -0.531012   \n",
       "4   -0.274313   -0.534803    0.797063   -0.156896    1.027882    1.130965   \n",
       "\n",
       "   Feature 98  Feature 99  \n",
       "0   -0.220330    1.140998  \n",
       "1   -0.306602   -0.855617  \n",
       "2    0.545067   -1.274210  \n",
       "3    0.249064   -1.877944  \n",
       "4    0.696718    0.098893  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Breeds: 120\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('features.csv')\n",
    "\n",
    "x = df.drop(columns = ['Breed'])\n",
    "y = df['Breed']\n",
    "num_classes = 120\n",
    "display(df.head())\n",
    "print(f\"Number of Breeds: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84336ead-25d3-401b-9e0e-311c35f29391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "# TEST_SIZE = 0.4\n",
    "# LEARN_RATE = 0.001\n",
    "# FIRST_LAYER = 128 \n",
    "# ACTIVATION_1 = 'relu'\n",
    "# SECOND_LAYER = 64\n",
    "# ACTIVATION_2 = 'relu'\n",
    "# ACTIVATION_OUT = 'softmax'\n",
    "# LOSS_TYPE = 'categorical_crossentropy'\n",
    "# METRICS = ['accuracy']\n",
    "# EPOCHS = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2cc74830-7385-4568-bab2-f09f4a41df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'first_layer': [2, 4, 8, 32, 64, 128],\n",
    "    'second_layer': [4, 16, 32, 64],\n",
    "    'activation_1': ['relu', 'tanh'],\n",
    "    'activation_2': ['relu', 'tanh'],\n",
    "    'activation_out': ['softmax'],\n",
    "    'learn_rate': [0.0001, 0.0005, 0.001, 0.005, .01],\n",
    "    'batch_size' : [10, 20, 50, 100]\n",
    "    'epochs' : [10, 20]\n",
    "}\n",
    "ACTIVATION_1 = 'relu'\n",
    "ACTIVATION_2 = 'relu'\n",
    "ACTIVATION_OUT = 'softmax'\n",
    "TEST_SIZE = 0.4\n",
    "EPOCHS = 20\n",
    "METRICS = ['accuracy']\n",
    "LOSS_TYPE = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be0facdc-6b55-44b7-9e58-cd09912f0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=['Breed'])\n",
    "y = df['Breed']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = TEST_SIZE)\n",
    "\n",
    "# Encode the categorical target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert the encoded labels to one-hot encoding\n",
    "y_train = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test_encoded, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b11822a6-d107-4ba2-b268-e31efe07dc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "383/383 [==============================] - 1s 2ms/step - loss: 2.0991 - accuracy: 0.5137 - val_loss: 0.9326 - val_accuracy: 0.7228\n",
      "Epoch 2/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.7194 - accuracy: 0.7833 - val_loss: 0.7441 - val_accuracy: 0.7644\n",
      "Epoch 3/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.5618 - accuracy: 0.8211 - val_loss: 0.7361 - val_accuracy: 0.7720\n",
      "Epoch 4/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.4772 - accuracy: 0.8469 - val_loss: 0.7419 - val_accuracy: 0.7783\n",
      "Epoch 5/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.4181 - accuracy: 0.8645 - val_loss: 0.7413 - val_accuracy: 0.7807\n",
      "Epoch 6/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.3675 - accuracy: 0.8799 - val_loss: 0.7536 - val_accuracy: 0.7785\n",
      "Epoch 7/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.3288 - accuracy: 0.8925 - val_loss: 0.7883 - val_accuracy: 0.7796\n",
      "Epoch 8/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.2905 - accuracy: 0.9045 - val_loss: 0.8196 - val_accuracy: 0.7714\n",
      "Epoch 9/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.2607 - accuracy: 0.9130 - val_loss: 0.8297 - val_accuracy: 0.7788\n",
      "Epoch 10/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.2359 - accuracy: 0.9214 - val_loss: 0.8721 - val_accuracy: 0.7745\n",
      "Epoch 11/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.2116 - accuracy: 0.9321 - val_loss: 0.8772 - val_accuracy: 0.7791\n",
      "Epoch 12/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.1850 - accuracy: 0.9401 - val_loss: 0.9256 - val_accuracy: 0.7779\n",
      "Epoch 13/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.1691 - accuracy: 0.9443 - val_loss: 0.9499 - val_accuracy: 0.7770\n",
      "Epoch 14/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.1498 - accuracy: 0.9512 - val_loss: 0.9969 - val_accuracy: 0.7730\n",
      "Epoch 15/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.1378 - accuracy: 0.9559 - val_loss: 1.0684 - val_accuracy: 0.7723\n",
      "Epoch 16/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.1195 - accuracy: 0.9611 - val_loss: 1.1025 - val_accuracy: 0.7687\n",
      "Epoch 17/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.1025 - accuracy: 0.9680 - val_loss: 1.1282 - val_accuracy: 0.7669\n",
      "Epoch 18/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.0914 - accuracy: 0.9720 - val_loss: 1.1369 - val_accuracy: 0.7718\n",
      "Epoch 19/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.0919 - accuracy: 0.9719 - val_loss: 1.2341 - val_accuracy: 0.7674\n",
      "Epoch 20/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.0916 - accuracy: 0.9697 - val_loss: 1.2570 - val_accuracy: 0.7646\n",
      "256/256 [==============================] - 0s 710us/step\n"
     ]
    }
   ],
   "source": [
    "def create_model(first_layer=128, activation_1='relu', second_layer=64, activation_2='relu', activation_out='softmax', learn_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(first_layer, input_shape=(100,), activation=activation_1))\n",
    "    model.add(Dense(second_layer, activation=activation_2))\n",
    "    model.add(Dense(num_classes, activation=activation_out))\n",
    "    model.compile(optimizer=Adam(learning_rate=learn_rate), loss=LOSS_TYPE, metrics=METRICS)\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=EPOCHS, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80d1b997-4437-4ad5-ba4e-008b127f9067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 726us/step - loss: 1.2570 - accuracy: 0.7646\n",
      "Test Accuracy: 0.7645618915557861\n",
      "Confusion Matrix for each label : \n",
      "[[[8110    5]\n",
      "  [   4   53]]\n",
      "\n",
      " [[8061   12]\n",
      "  [  16   83]]\n",
      "\n",
      " [[8105    6]\n",
      "  [   5   56]]\n",
      "\n",
      " [[8074   23]\n",
      "  [  16   59]]\n",
      "\n",
      " [[8089   16]\n",
      "  [  34   33]]\n",
      "\n",
      " [[8067   40]\n",
      "  [  20   45]]\n",
      "\n",
      " [[8079    8]\n",
      "  [  36   49]]\n",
      "\n",
      " [[8072   13]\n",
      "  [  20   67]]\n",
      "\n",
      " [[8084    8]\n",
      "  [  20   60]]\n",
      "\n",
      " [[8076   19]\n",
      "  [  20   57]]\n",
      "\n",
      " [[8091   10]\n",
      "  [   3   68]]\n",
      "\n",
      " [[8075   14]\n",
      "  [   9   74]]\n",
      "\n",
      " [[8094   15]\n",
      "  [   8   55]]\n",
      "\n",
      " [[8092   12]\n",
      "  [  13   55]]\n",
      "\n",
      " [[8081   16]\n",
      "  [  10   65]]\n",
      "\n",
      " [[8091   16]\n",
      "  [  38   27]]\n",
      "\n",
      " [[8102    5]\n",
      "  [  11   54]]\n",
      "\n",
      " [[8098    6]\n",
      "  [  13   55]]\n",
      "\n",
      " [[8096    8]\n",
      "  [  14   54]]\n",
      "\n",
      " [[8099   15]\n",
      "  [  12   46]]\n",
      "\n",
      " [[8094   12]\n",
      "  [  15   51]]\n",
      "\n",
      " [[8113    7]\n",
      "  [   6   46]]\n",
      "\n",
      " [[8093   19]\n",
      "  [  17   43]]\n",
      "\n",
      " [[8097    9]\n",
      "  [  17   49]]\n",
      "\n",
      " [[8104    7]\n",
      "  [  11   50]]\n",
      "\n",
      " [[8082   17]\n",
      "  [  18   55]]\n",
      "\n",
      " [[8097    9]\n",
      "  [  21   45]]\n",
      "\n",
      " [[8102    7]\n",
      "  [  17   46]]\n",
      "\n",
      " [[8084   23]\n",
      "  [  18   47]]\n",
      "\n",
      " [[8106    1]\n",
      "  [   8   57]]\n",
      "\n",
      " [[8118    3]\n",
      "  [   8   43]]\n",
      "\n",
      " [[8100    9]\n",
      "  [  18   45]]\n",
      "\n",
      " [[8088   14]\n",
      "  [  13   57]]\n",
      "\n",
      " [[8063   47]\n",
      "  [  10   52]]\n",
      "\n",
      " [[8068   39]\n",
      "  [  43   22]]\n",
      "\n",
      " [[8093   16]\n",
      "  [   4   59]]\n",
      "\n",
      " [[8113    0]\n",
      "  [   6   53]]\n",
      "\n",
      " [[8097   10]\n",
      "  [  16   49]]\n",
      "\n",
      " [[8102    6]\n",
      "  [  21   43]]\n",
      "\n",
      " [[8099   19]\n",
      "  [  15   39]]\n",
      "\n",
      " [[8110    1]\n",
      "  [  29   32]]\n",
      "\n",
      " [[8090   18]\n",
      "  [   9   55]]\n",
      "\n",
      " [[8078   12]\n",
      "  [  27   55]]\n",
      "\n",
      " [[8094   19]\n",
      "  [  42   17]]\n",
      "\n",
      " [[8092   21]\n",
      "  [   9   50]]\n",
      "\n",
      " [[8099   17]\n",
      "  [  19   37]]\n",
      "\n",
      " [[8084   32]\n",
      "  [  16   40]]\n",
      "\n",
      " [[8108   11]\n",
      "  [   4   49]]\n",
      "\n",
      " [[8103    5]\n",
      "  [   7   57]]\n",
      "\n",
      " [[8096   10]\n",
      "  [  26   40]]\n",
      "\n",
      " [[8070   15]\n",
      "  [  23   64]]\n",
      "\n",
      " [[8082   14]\n",
      "  [  21   55]]\n",
      "\n",
      " [[8103   11]\n",
      "  [   5   53]]\n",
      "\n",
      " [[8091   23]\n",
      "  [   9   49]]\n",
      "\n",
      " [[8108   13]\n",
      "  [   7   44]]\n",
      "\n",
      " [[8081   20]\n",
      "  [   8   63]]\n",
      "\n",
      " [[8099    9]\n",
      "  [   5   59]]\n",
      "\n",
      " [[8066   30]\n",
      "  [  15   61]]\n",
      "\n",
      " [[8090   14]\n",
      "  [   7   61]]\n",
      "\n",
      " [[8068   10]\n",
      "  [  35   59]]\n",
      "\n",
      " [[8074   24]\n",
      "  [  15   59]]\n",
      "\n",
      " [[8086    6]\n",
      "  [  10   70]]\n",
      "\n",
      " [[8106    0]\n",
      "  [   3   63]]\n",
      "\n",
      " [[8093   12]\n",
      "  [  28   39]]\n",
      "\n",
      " [[8089    5]\n",
      "  [  12   66]]\n",
      "\n",
      " [[8106    5]\n",
      "  [   3   58]]\n",
      "\n",
      " [[8101   17]\n",
      "  [  13   41]]\n",
      "\n",
      " [[8086   16]\n",
      "  [  28   42]]\n",
      "\n",
      " [[8071   14]\n",
      "  [  38   49]]\n",
      "\n",
      " [[8091    5]\n",
      "  [   7   69]]\n",
      "\n",
      " [[8072   26]\n",
      "  [  44   30]]\n",
      "\n",
      " [[8075   28]\n",
      "  [  17   52]]\n",
      "\n",
      " [[8079   19]\n",
      "  [  10   64]]\n",
      "\n",
      " [[8047   36]\n",
      "  [  11   78]]\n",
      "\n",
      " [[8114    6]\n",
      "  [   2   50]]\n",
      "\n",
      " [[8071   22]\n",
      "  [  16   63]]\n",
      "\n",
      " [[8074   42]\n",
      "  [  29   27]]\n",
      "\n",
      " [[8116    3]\n",
      "  [  29   24]]\n",
      "\n",
      " [[8063   29]\n",
      "  [  17   63]]\n",
      "\n",
      " [[8069   33]\n",
      "  [  22   48]]\n",
      "\n",
      " [[8077   10]\n",
      "  [   5   80]]\n",
      "\n",
      " [[8076   11]\n",
      "  [  33   52]]\n",
      "\n",
      " [[8109    4]\n",
      "  [  11   48]]\n",
      "\n",
      " [[8101   16]\n",
      "  [   9   46]]\n",
      "\n",
      " [[8084    9]\n",
      "  [   7   72]]\n",
      "\n",
      " [[8101   12]\n",
      "  [  10   49]]\n",
      "\n",
      " [[8083   17]\n",
      "  [   9   63]]\n",
      "\n",
      " [[8065    8]\n",
      "  [   7   92]]\n",
      "\n",
      " [[8078   19]\n",
      "  [   4   71]]\n",
      "\n",
      " [[8062   49]\n",
      "  [  12   49]]\n",
      "\n",
      " [[8078   27]\n",
      "  [  32   35]]\n",
      "\n",
      " [[8104   11]\n",
      "  [   7   50]]\n",
      "\n",
      " [[8098    5]\n",
      "  [   3   66]]\n",
      "\n",
      " [[8090    2]\n",
      "  [  23   57]]\n",
      "\n",
      " [[8086   10]\n",
      "  [   7   69]]\n",
      "\n",
      " [[8088    9]\n",
      "  [  10   65]]\n",
      "\n",
      " [[8103    7]\n",
      "  [  12   50]]\n",
      "\n",
      " [[8062   28]\n",
      "  [   8   74]]\n",
      "\n",
      " [[8084    5]\n",
      "  [  13   70]]\n",
      "\n",
      " [[8073   35]\n",
      "  [  18   46]]\n",
      "\n",
      " [[8055   38]\n",
      "  [  30   49]]\n",
      "\n",
      " [[8077   28]\n",
      "  [  19   48]]\n",
      "\n",
      " [[8081   35]\n",
      "  [  17   39]]\n",
      "\n",
      " [[8092   20]\n",
      "  [  19   41]]\n",
      "\n",
      " [[8068   36]\n",
      "  [  14   54]]\n",
      "\n",
      " [[8109    2]\n",
      "  [  10   51]]\n",
      "\n",
      " [[8090   13]\n",
      "  [  11   58]]\n",
      "\n",
      " [[8085   24]\n",
      "  [   9   54]]\n",
      "\n",
      " [[8042   43]\n",
      "  [  19   68]]\n",
      "\n",
      " [[8083   21]\n",
      "  [  33   35]]\n",
      "\n",
      " [[8080   11]\n",
      "  [  24   57]]\n",
      "\n",
      " [[8061   19]\n",
      "  [  44   48]]\n",
      "\n",
      " [[8096   11]\n",
      "  [  22   43]]\n",
      "\n",
      " [[8096   19]\n",
      "  [  30   27]]\n",
      "\n",
      " [[8096   15]\n",
      "  [   8   53]]\n",
      "\n",
      " [[8101   12]\n",
      "  [  15   44]]\n",
      "\n",
      " [[8086   13]\n",
      "  [  16   57]]\n",
      "\n",
      " [[8048   46]\n",
      "  [  22   56]]\n",
      "\n",
      " [[8080   30]\n",
      "  [  21   41]]]\n",
      "Classification Report : \n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                 affenpinscher       0.91      0.93      0.92        57\n",
      "                  afghan_hound       0.87      0.84      0.86        99\n",
      "           african_hunting_dog       0.90      0.92      0.91        61\n",
      "                      airedale       0.72      0.79      0.75        75\n",
      "american_staffordshire_terrier       0.67      0.49      0.57        67\n",
      "                   appenzeller       0.53      0.69      0.60        65\n",
      "            australian_terrier       0.86      0.58      0.69        85\n",
      "                       basenji       0.84      0.77      0.80        87\n",
      "                        basset       0.88      0.75      0.81        80\n",
      "                        beagle       0.75      0.74      0.75        77\n",
      "            bedlington_terrier       0.87      0.96      0.91        71\n",
      "          bernese_mountain_dog       0.84      0.89      0.87        83\n",
      "              blenheim_spaniel       0.79      0.87      0.83        63\n",
      "                    bloodhound       0.82      0.81      0.81        68\n",
      "                      bluetick       0.80      0.87      0.83        75\n",
      "                 border_collie       0.63      0.42      0.50        65\n",
      "                border_terrier       0.92      0.83      0.87        65\n",
      "                        borzoi       0.90      0.81      0.85        68\n",
      "                   boston_bull       0.87      0.79      0.83        68\n",
      "          bouvier_des_flandres       0.75      0.79      0.77        58\n",
      "                         boxer       0.81      0.77      0.79        66\n",
      "             brabancon_griffon       0.87      0.88      0.88        52\n",
      "                        briard       0.69      0.72      0.70        60\n",
      "              brittany_spaniel       0.84      0.74      0.79        66\n",
      "                  bull_mastiff       0.88      0.82      0.85        61\n",
      "                         cairn       0.76      0.75      0.76        73\n",
      "                      cardigan       0.83      0.68      0.75        66\n",
      "      chesapeake_bay_retriever       0.87      0.73      0.79        63\n",
      "                     chihuahua       0.67      0.72      0.70        65\n",
      "                          chow       0.98      0.88      0.93        65\n",
      "                       clumber       0.93      0.84      0.89        51\n",
      "              coated_retriever       0.83      0.71      0.77        63\n",
      "        coated_wheaten_terrier       0.80      0.81      0.81        70\n",
      "                cocker_spaniel       0.53      0.84      0.65        62\n",
      "                        collie       0.36      0.34      0.35        65\n",
      "                dandie_dinmont       0.79      0.94      0.86        63\n",
      "                         dhole       1.00      0.90      0.95        59\n",
      "                         dingo       0.83      0.75      0.79        65\n",
      "                      doberman       0.88      0.67      0.76        64\n",
      "              english_foxhound       0.67      0.72      0.70        54\n",
      "                english_setter       0.97      0.52      0.68        61\n",
      "              english_springer       0.75      0.86      0.80        64\n",
      "                   entlebucher       0.82      0.67      0.74        82\n",
      "                    eskimo_dog       0.47      0.29      0.36        59\n",
      "                french_bulldog       0.70      0.85      0.77        59\n",
      "               german_shepherd       0.69      0.66      0.67        56\n",
      "               giant_schnauzer       0.56      0.71      0.62        56\n",
      "              golden_retriever       0.82      0.92      0.87        53\n",
      "                 gordon_setter       0.92      0.89      0.90        64\n",
      "                    great_dane       0.80      0.61      0.69        66\n",
      "                great_pyrenees       0.81      0.74      0.77        87\n",
      "    greater_swiss_mountain_dog       0.80      0.72      0.76        76\n",
      "                   groenendael       0.83      0.91      0.87        58\n",
      "            haired_fox_terrier       0.68      0.84      0.75        58\n",
      "                haired_pointer       0.77      0.86      0.81        51\n",
      "                  ibizan_hound       0.76      0.89      0.82        71\n",
      "                  irish_setter       0.87      0.92      0.89        64\n",
      "                 irish_terrier       0.67      0.80      0.73        76\n",
      "           irish_water_spaniel       0.81      0.90      0.85        68\n",
      "               irish_wolfhound       0.86      0.63      0.72        94\n",
      "             italian_greyhound       0.71      0.80      0.75        74\n",
      "              japanese_spaniel       0.92      0.88      0.90        80\n",
      "                      keeshond       1.00      0.95      0.98        66\n",
      "                        kelpie       0.76      0.58      0.66        67\n",
      "            kerry_blue_terrier       0.93      0.85      0.89        78\n",
      "                      komondor       0.92      0.95      0.94        61\n",
      "                        kuvasz       0.71      0.76      0.73        54\n",
      "            labrador_retriever       0.72      0.60      0.66        70\n",
      "              lakeland_terrier       0.78      0.56      0.65        87\n",
      "                      leonberg       0.93      0.91      0.92        76\n",
      "                         lhasa       0.54      0.41      0.46        74\n",
      "                      malamute       0.65      0.75      0.70        69\n",
      "                      malinois       0.77      0.86      0.82        74\n",
      "                   maltese_dog       0.68      0.88      0.77        89\n",
      "              mexican_hairless       0.89      0.96      0.93        52\n",
      "            miniature_pinscher       0.74      0.80      0.77        79\n",
      "              miniature_poodle       0.39      0.48      0.43        56\n",
      "           miniature_schnauzer       0.89      0.45      0.60        53\n",
      "                  newfoundland       0.68      0.79      0.73        80\n",
      "               norfolk_terrier       0.59      0.69      0.64        70\n",
      "            norwegian_elkhound       0.89      0.94      0.91        85\n",
      "               norwich_terrier       0.83      0.61      0.70        85\n",
      "          old_english_sheepdog       0.92      0.81      0.86        59\n",
      "                    otterhound       0.74      0.84      0.79        55\n",
      "                      papillon       0.89      0.91      0.90        79\n",
      "                      pekinese       0.80      0.83      0.82        59\n",
      "                      pembroke       0.79      0.88      0.83        72\n",
      "                    pomeranian       0.92      0.93      0.92        99\n",
      "                           pug       0.79      0.95      0.86        75\n",
      "                       redbone       0.50      0.80      0.62        61\n",
      "           rhodesian_ridgeback       0.56      0.52      0.54        67\n",
      "                    rottweiler       0.82      0.88      0.85        57\n",
      "                 saint_bernard       0.93      0.96      0.94        69\n",
      "                        saluki       0.97      0.71      0.82        80\n",
      "                       samoyed       0.87      0.91      0.89        76\n",
      "                    schipperke       0.88      0.87      0.87        75\n",
      "                scotch_terrier       0.88      0.81      0.84        62\n",
      "            scottish_deerhound       0.73      0.90      0.80        82\n",
      "              sealyham_terrier       0.93      0.84      0.89        83\n",
      "             shetland_sheepdog       0.57      0.72      0.63        64\n",
      "                siberian_husky       0.56      0.62      0.59        79\n",
      "                 silky_terrier       0.63      0.72      0.67        67\n",
      "     staffordshire_bullterrier       0.53      0.70      0.60        56\n",
      "               standard_poodle       0.67      0.68      0.68        60\n",
      "            standard_schnauzer       0.60      0.79      0.68        68\n",
      "                sussex_spaniel       0.96      0.84      0.89        61\n",
      "                 tan_coonhound       0.82      0.84      0.83        69\n",
      "               tibetan_mastiff       0.69      0.86      0.77        63\n",
      "               tibetan_terrier       0.61      0.78      0.69        87\n",
      "                    toy_poodle       0.62      0.51      0.56        68\n",
      "                   toy_terrier       0.84      0.70      0.77        81\n",
      "                           tzu       0.72      0.52      0.60        92\n",
      "                        vizsla       0.80      0.66      0.72        65\n",
      "                  walker_hound       0.59      0.47      0.52        57\n",
      "                    weimaraner       0.78      0.87      0.82        61\n",
      "        welsh_springer_spaniel       0.79      0.75      0.77        59\n",
      "   west_highland_white_terrier       0.81      0.78      0.80        73\n",
      "                       whippet       0.55      0.72      0.62        78\n",
      "             yorkshire_terrier       0.58      0.66      0.62        62\n",
      "\n",
      "                      accuracy                           0.76      8172\n",
      "                     macro avg       0.77      0.76      0.76      8172\n",
      "                  weighted avg       0.77      0.76      0.76      8172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert continuous predictions to class labels\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert one-hot encoded y_test back to class labels\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Inverse transform the predicted and ground truth class labels to original breed names\n",
    "y_pred_breed = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_test_breed = label_encoder.inverse_transform(y_test_classes)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "#print(\"Accuracy : \", accuracy_score(y_test, y_pred))\n",
    "#print(\"Mean Square Error : \", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix for each label : \")\n",
    "print(multilabel_confusion_matrix(y_test_breed, y_pred_breed))\n",
    "\n",
    "print(\"Classification Report : \")\n",
    "print(classification_report(y_test_breed, y_pred_breed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78860532-2cbd-4112-b51c-f0de5f5c1539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 636us/step\n",
      "256/256 [==============================] - 0s 580us/step\n",
      "256/256 [==============================] - 0s 682us/step\n",
      "256/256 [==============================] - 0s 608us/step\n",
      "256/256 [==============================] - 0s 624us/step\n",
      "256/256 [==============================] - 0s 655us/step\n",
      "256/256 [==============================] - 0s 557us/step\n",
      "256/256 [==============================] - 0s 620us/step\n",
      "256/256 [==============================] - 0s 642us/step\n",
      "256/256 [==============================] - 0s 592us/step\n",
      "256/256 [==============================] - 0s 592us/step\n",
      "256/256 [==============================] - 0s 576us/step\n",
      "256/256 [==============================] - 0s 588us/step\n",
      "256/256 [==============================] - 0s 588us/step\n",
      "256/256 [==============================] - 0s 642us/step\n",
      "256/256 [==============================] - 0s 616us/step\n",
      "256/256 [==============================] - 0s 604us/step\n",
      "256/256 [==============================] - 0s 561us/step\n",
      "256/256 [==============================] - 0s 584us/step\n",
      "256/256 [==============================] - 0s 576us/step\n",
      "256/256 [==============================] - 0s 639us/step\n",
      "256/256 [==============================] - 0s 620us/step\n",
      "256/256 [==============================] - 0s 667us/step\n",
      "256/256 [==============================] - 0s 624us/step\n",
      "256/256 [==============================] - 0s 580us/step\n",
      "256/256 [==============================] - 0s 586us/step\n",
      "256/256 [==============================] - 0s 639us/step\n",
      "256/256 [==============================] - 0s 588us/step\n",
      "256/256 [==============================] - 0s 647us/step\n",
      "256/256 [==============================] - 0s 600us/step\n",
      "256/256 [==============================] - 0s 631us/step\n",
      "256/256 [==============================] - 0s 729us/step\n",
      "256/256 [==============================] - 0s 682us/step\n",
      "256/256 [==============================] - 0s 651us/step\n",
      "256/256 [==============================] - 0s 624us/step\n",
      "256/256 [==============================] - 0s 643us/step\n",
      "256/256 [==============================] - 0s 596us/step\n",
      "256/256 [==============================] - 0s 596us/step\n",
      "256/256 [==============================] - 0s 667us/step\n",
      "256/256 [==============================] - 0s 612us/step\n",
      "256/256 [==============================] - 0s 714us/step\n",
      "256/256 [==============================] - 0s 569us/step\n",
      "256/256 [==============================] - 0s 631us/step\n",
      "256/256 [==============================] - 0s 584us/step\n",
      "256/256 [==============================] - 0s 725us/step\n",
      "256/256 [==============================] - 0s 584us/step\n",
      "256/256 [==============================] - 0s 584us/step\n",
      "256/256 [==============================] - 0s 573us/step\n",
      "256/256 [==============================] - 0s 643us/step\n",
      "256/256 [==============================] - 0s 604us/step\n",
      "256/256 [==============================] - 0s 647us/step\n",
      "256/256 [==============================] - 0s 573us/step\n",
      "256/256 [==============================] - 0s 620us/step\n",
      "256/256 [==============================] - 0s 569us/step\n",
      "256/256 [==============================] - 0s 659us/step\n",
      "256/256 [==============================] - 0s 702us/step\n",
      "256/256 [==============================] - 0s 620us/step\n",
      "256/256 [==============================] - 0s 596us/step\n",
      "256/256 [==============================] - 0s 612us/step\n",
      "256/256 [==============================] - 0s 576us/step\n",
      "256/256 [==============================] - 0s 580us/step\n",
      "256/256 [==============================] - 0s 698us/step\n",
      "256/256 [==============================] - 0s 655us/step\n",
      "256/256 [==============================] - 0s 596us/step\n",
      "256/256 [==============================] - 0s 659us/step\n",
      "256/256 [==============================] - 0s 655us/step\n",
      "256/256 [==============================] - 0s 580us/step\n",
      "256/256 [==============================] - 0s 646us/step\n",
      "256/256 [==============================] - 0s 639us/step\n",
      "256/256 [==============================] - 0s 631us/step\n",
      "256/256 [==============================] - 0s 627us/step\n",
      "256/256 [==============================] - 0s 577us/step\n",
      "256/256 [==============================] - 0s 600us/step\n",
      "256/256 [==============================] - 0s 635us/step\n",
      "256/256 [==============================] - 0s 605us/step\n",
      "256/256 [==============================] - 0s 588us/step\n",
      "256/256 [==============================] - 0s 616us/step\n",
      "256/256 [==============================] - 0s 682us/step\n",
      "256/256 [==============================] - 0s 635us/step\n",
      "256/256 [==============================] - 0s 765us/step\n",
      "256/256 [==============================] - 0s 631us/step\n",
      "256/256 [==============================] - 0s 643us/step\n",
      "256/256 [==============================] - 0s 608us/step\n",
      "256/256 [==============================] - 0s 600us/step\n",
      "256/256 [==============================] - 0s 771us/step\n",
      "256/256 [==============================] - 0s 616us/step\n",
      "256/256 [==============================] - 0s 651us/step\n",
      "256/256 [==============================] - 0s 627us/step\n",
      "256/256 [==============================] - 0s 812us/step\n",
      "256/256 [==============================] - 0s 643us/step\n",
      "256/256 [==============================] - 0s 565us/step\n",
      "256/256 [==============================] - 0s 702us/step\n",
      "256/256 [==============================] - 0s 651us/step\n",
      "256/256 [==============================] - 0s 580us/step\n",
      "256/256 [==============================] - 0s 592us/step\n",
      "256/256 [==============================] - 0s 708us/step\n",
      "256/256 [==============================] - 0s 569us/step\n",
      "256/256 [==============================] - 0s 616us/step\n",
      "256/256 [==============================] - 0s 569us/step\n",
      "256/256 [==============================] - 0s 622us/step\n",
      "256/256 [==============================] - 0s 596us/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_260848\\2161367385.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbest_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Usage example:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mbest_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperform_grid_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best accuracy: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbest_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best parameters:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_260848\\2161367385.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x_train, y_train, x_test, y_test, param_grid)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mACTIVATION_OUT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# num_classes is the number of unique breed labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLOSS_TYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMETRICS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1794\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1795\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1796\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1797\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1798\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1799\u001b[0m                         with tf.profiler.experimental.Trace(\n\u001b[0;32m   1800\u001b[0m                             \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1801\u001b[0m                             \u001b[0mepoch_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1407\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_step\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m         ):\n\u001b[0;32m   1409\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1410\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1411\u001b[1;33m             \u001b[0moriginal_spe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1412\u001b[0m             can_run_full_execution = (\n\u001b[0;32m   1413\u001b[0m                 \u001b[0moriginal_spe\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    687\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    691\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    838\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m       \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[1;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m     \u001b[1;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 842\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    308\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"graph\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;31m# Make sure we get an input with handle data attached from resource\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_data\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   4185\u001b[0m         _ctx, \"Identity\", name, input)\n\u001b[0;32m   4186\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4187\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4188\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4189\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4190\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4191\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4192\u001b[0m       return identity_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def perform_grid_search(x_train, y_train, x_test, y_test, param_grid):\n",
    "    best_accuracy = 0\n",
    "    best_params = {}\n",
    "\n",
    "    # Iterate over all combinations of hyperparameters\n",
    "    for batch_size, epoch, first_layer, second_layer, learn_rate in itertools.product(param_grid['batch_size'], param_grid['epochs'], param_grid['first_layer'], param_grid['second_layer'], param_grid['learn_rate']):\n",
    "        # Build the model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(first_layer, input_shape=(100,), activation=ACTIVATION_1))  # Assuming 100 features\n",
    "        model.add(Dense(second_layer, activation=ACTIVATION_2))\n",
    "        model.add(Dense(num_classes, activation=ACTIVATION_OUT))  # num_classes is the number of unique breed labels\n",
    "        model.compile(optimizer=Adam(learning_rate=learn_rate), loss=LOSS_TYPE, metrics=METRICS)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(x_train, y_train, epochs=epoch, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        y_pred = model.predict(x_test)\n",
    "        accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "        \n",
    "        # Check if current combination is better than the previous best\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = {\n",
    "                'batch_size': batch_size,\n",
    "                'epochs': epoch,\n",
    "                'first_layer': first_layer,\n",
    "                'second_layer': second_layer,\n",
    "                'learn_rate': learn_rate\n",
    "            }\n",
    "\n",
    "    return best_accuracy, best_params\n",
    "\n",
    "# Usage example:\n",
    "best_accuracy, best_params = perform_grid_search(x_train, y_train, x_test, y_test, param_grid)\n",
    "print(\"Best accuracy: %f\" % best_accuracy)\n",
    "print(\"Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c3baa-a384-4601-9d6f-51b3fd5e0a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
