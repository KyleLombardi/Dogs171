{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c480d74a-d567-46ac-812c-a790df98caa7",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Network\n",
    "\n",
    "This file is used to run the optimally-determined model as per grid search in FFNN_Grid_Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f7599a31-d302-46c6-a558-35b7d3a2ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a3839974-e82f-43f6-9445-7da6e263123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikeras in c:\\users\\kirin\\miniconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: keras in c:\\users\\kirin\\miniconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: packaging>=0.21 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikeras) (23.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikeras) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem<0.32,>=0.23.1 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikeras) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikeras keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dfd374a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ebf9798b-aa96-4398-a317-e1cbad5518ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(first_layer=128, activation_1='relu', second_layer=64, activation_2='relu', activation_out='softmax', learn_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(first_layer, input_shape=(100,), activation=activation_1))\n",
    "    model.add(Dense(second_layer, activation=activation_2))\n",
    "    model.add(Dense(num_classes, activation=activation_out))\n",
    "    model.compile(optimizer=Adam(learning_rate=learn_rate), loss=LOSS_TYPE, metrics=METRICS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "84336ead-25d3-401b-9e0e-311c35f29391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "307/307 [==============================] - 1s 2ms/step - loss: 2.8862 - accuracy: 0.3688 - val_loss: 1.2868 - val_accuracy: 0.6640\n",
      "Epoch 2/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.9486 - accuracy: 0.7323 - val_loss: 0.8754 - val_accuracy: 0.7355\n",
      "Epoch 3/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.7039 - accuracy: 0.7883 - val_loss: 0.7669 - val_accuracy: 0.7674\n",
      "Epoch 4/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.6015 - accuracy: 0.8159 - val_loss: 0.7291 - val_accuracy: 0.7819\n",
      "Epoch 5/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.5366 - accuracy: 0.8336 - val_loss: 0.7162 - val_accuracy: 0.7832\n",
      "Epoch 6/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.4910 - accuracy: 0.8445 - val_loss: 0.6987 - val_accuracy: 0.7852\n",
      "Epoch 7/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.4529 - accuracy: 0.8565 - val_loss: 0.6917 - val_accuracy: 0.7909\n",
      "Epoch 8/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.4217 - accuracy: 0.8688 - val_loss: 0.6843 - val_accuracy: 0.7901\n",
      "Epoch 9/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.3934 - accuracy: 0.8784 - val_loss: 0.6868 - val_accuracy: 0.7901\n",
      "Epoch 10/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.3699 - accuracy: 0.8845 - val_loss: 0.6962 - val_accuracy: 0.7877\n",
      "Epoch 11/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.3471 - accuracy: 0.8924 - val_loss: 0.7067 - val_accuracy: 0.7885\n",
      "Epoch 12/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.3282 - accuracy: 0.8977 - val_loss: 0.7031 - val_accuracy: 0.7960\n",
      "Epoch 13/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.3112 - accuracy: 0.9056 - val_loss: 0.7186 - val_accuracy: 0.7901\n",
      "Epoch 14/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.2931 - accuracy: 0.9080 - val_loss: 0.7352 - val_accuracy: 0.7893\n",
      "Epoch 15/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.2765 - accuracy: 0.9140 - val_loss: 0.7359 - val_accuracy: 0.7907\n",
      "Epoch 16/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.2622 - accuracy: 0.9223 - val_loss: 0.7711 - val_accuracy: 0.7848\n",
      "Epoch 17/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.2475 - accuracy: 0.9272 - val_loss: 0.7569 - val_accuracy: 0.7877\n",
      "Epoch 18/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.2315 - accuracy: 0.9313 - val_loss: 0.7694 - val_accuracy: 0.7868\n",
      "Epoch 19/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.2206 - accuracy: 0.9345 - val_loss: 0.7876 - val_accuracy: 0.7868\n",
      "Epoch 20/20\n",
      "307/307 [==============================] - 0s 1ms/step - loss: 0.2087 - accuracy: 0.9393 - val_loss: 0.8170 - val_accuracy: 0.7823\n",
      "160/160 [==============================] - 0s 663us/step\n",
      "160/160 [==============================] - 0s 831us/step - loss: 0.5709 - accuracy: 0.8888\n",
      "Test Accuracy: 0.8887801170349121\n",
      "Confusion Matrix for each label : \n",
      "[[[5073    3]\n",
      "  [   5   26]]\n",
      "\n",
      " [[5053    2]\n",
      "  [   6   46]]\n",
      "\n",
      " [[5060    2]\n",
      "  [   1   44]]\n",
      "\n",
      " [[5059    4]\n",
      "  [   4   40]]\n",
      "\n",
      " [[5053    8]\n",
      "  [  10   36]]\n",
      "\n",
      " [[5067    8]\n",
      "  [   7   25]]\n",
      "\n",
      " [[5057    6]\n",
      "  [  12   32]]\n",
      "\n",
      " [[5056    2]\n",
      "  [   5   44]]\n",
      "\n",
      " [[5058    3]\n",
      "  [   4   42]]\n",
      "\n",
      " [[5060    6]\n",
      "  [   6   35]]\n",
      "\n",
      " [[5046    3]\n",
      "  [   1   57]]\n",
      "\n",
      " [[5057    5]\n",
      "  [   0   45]]\n",
      "\n",
      " [[5062    4]\n",
      "  [   0   41]]\n",
      "\n",
      " [[5053    3]\n",
      "  [   9   42]]\n",
      "\n",
      " [[5056    4]\n",
      "  [   3   44]]\n",
      "\n",
      " [[5062    5]\n",
      "  [   6   34]]\n",
      "\n",
      " [[5062    1]\n",
      "  [   7   37]]\n",
      "\n",
      " [[5051    4]\n",
      "  [   8   44]]\n",
      "\n",
      " [[5041   12]\n",
      "  [   0   54]]\n",
      "\n",
      " [[5053    7]\n",
      "  [   8   39]]\n",
      "\n",
      " [[5071    6]\n",
      "  [   2   28]]\n",
      "\n",
      " [[5058    5]\n",
      "  [   4   40]]\n",
      "\n",
      " [[5064    7]\n",
      "  [   3   33]]\n",
      "\n",
      " [[5057    3]\n",
      "  [   7   40]]\n",
      "\n",
      " [[5057    8]\n",
      "  [   2   40]]\n",
      "\n",
      " [[5055    6]\n",
      "  [   4   42]]\n",
      "\n",
      " [[5048   15]\n",
      "  [   2   42]]\n",
      "\n",
      " [[5051    5]\n",
      "  [   4   47]]\n",
      "\n",
      " [[5055   10]\n",
      "  [   6   36]]\n",
      "\n",
      " [[5062    1]\n",
      "  [   2   42]]\n",
      "\n",
      " [[5066    1]\n",
      "  [   2   38]]\n",
      "\n",
      " [[5062    3]\n",
      "  [   6   36]]\n",
      "\n",
      " [[5066    8]\n",
      "  [   3   30]]\n",
      "\n",
      " [[5052    7]\n",
      "  [   8   40]]\n",
      "\n",
      " [[5072    3]\n",
      "  [  14   18]]\n",
      "\n",
      " [[5051    2]\n",
      "  [   2   52]]\n",
      "\n",
      " [[5073    1]\n",
      "  [   3   30]]\n",
      "\n",
      " [[5071    3]\n",
      "  [   5   28]]\n",
      "\n",
      " [[5073    2]\n",
      "  [   2   30]]\n",
      "\n",
      " [[5056   12]\n",
      "  [   8   31]]\n",
      "\n",
      " [[5068    3]\n",
      "  [   6   30]]\n",
      "\n",
      " [[5068    1]\n",
      "  [   2   36]]\n",
      "\n",
      " [[5059    3]\n",
      "  [   5   40]]\n",
      "\n",
      " [[5063    5]\n",
      "  [  13   26]]\n",
      "\n",
      " [[5073    2]\n",
      "  [   5   27]]\n",
      "\n",
      " [[5060    6]\n",
      "  [   6   35]]\n",
      "\n",
      " [[5058    5]\n",
      "  [   4   40]]\n",
      "\n",
      " [[5068    5]\n",
      "  [   3   31]]\n",
      "\n",
      " [[5066    1]\n",
      "  [   1   39]]\n",
      "\n",
      " [[5052    9]\n",
      "  [   3   43]]\n",
      "\n",
      " [[5049    4]\n",
      "  [   7   47]]\n",
      "\n",
      " [[5058    2]\n",
      "  [   6   41]]\n",
      "\n",
      " [[5064    3]\n",
      "  [   2   38]]\n",
      "\n",
      " [[5061    7]\n",
      "  [   5   34]]\n",
      "\n",
      " [[5070    3]\n",
      "  [   2   32]]\n",
      "\n",
      " [[5050    6]\n",
      "  [   5   46]]\n",
      "\n",
      " [[5067    1]\n",
      "  [   1   38]]\n",
      "\n",
      " [[5058    4]\n",
      "  [   4   41]]\n",
      "\n",
      " [[5073    2]\n",
      "  [   1   31]]\n",
      "\n",
      " [[5045    7]\n",
      "  [   5   50]]\n",
      "\n",
      " [[5058    7]\n",
      "  [   5   37]]\n",
      "\n",
      " [[5050    1]\n",
      "  [   4   52]]\n",
      "\n",
      " [[5069    3]\n",
      "  [   0   35]]\n",
      "\n",
      " [[5071    3]\n",
      "  [   5   28]]\n",
      "\n",
      " [[5057    5]\n",
      "  [   1   44]]\n",
      "\n",
      " [[5071    1]\n",
      "  [   2   33]]\n",
      "\n",
      " [[5073    4]\n",
      "  [   3   27]]\n",
      "\n",
      " [[5055    8]\n",
      "  [   5   39]]\n",
      "\n",
      " [[5052    6]\n",
      "  [  10   39]]\n",
      "\n",
      " [[5045    1]\n",
      "  [   2   59]]\n",
      "\n",
      " [[5054   11]\n",
      "  [  10   32]]\n",
      "\n",
      " [[5050    5]\n",
      "  [  13   39]]\n",
      "\n",
      " [[5062    6]\n",
      "  [   4   35]]\n",
      "\n",
      " [[5032    3]\n",
      "  [   4   68]]\n",
      "\n",
      " [[5076    2]\n",
      "  [   2   27]]\n",
      "\n",
      " [[5062    6]\n",
      "  [   2   37]]\n",
      "\n",
      " [[5059    9]\n",
      "  [  13   26]]\n",
      "\n",
      " [[5063    7]\n",
      "  [   6   31]]\n",
      "\n",
      " [[5069    2]\n",
      "  [   5   31]]\n",
      "\n",
      " [[5054   11]\n",
      "  [   2   40]]\n",
      "\n",
      " [[5061    3]\n",
      "  [   1   42]]\n",
      "\n",
      " [[5056    4]\n",
      "  [   8   39]]\n",
      "\n",
      " [[5058    2]\n",
      "  [   3   44]]\n",
      "\n",
      " [[5063    0]\n",
      "  [   5   39]]\n",
      "\n",
      " [[5058    1]\n",
      "  [   3   45]]\n",
      "\n",
      " [[5055    7]\n",
      "  [   1   44]]\n",
      "\n",
      " [[5056    1]\n",
      "  [  12   38]]\n",
      "\n",
      " [[5056    1]\n",
      "  [   1   49]]\n",
      "\n",
      " [[5055    4]\n",
      "  [   5   43]]\n",
      "\n",
      " [[5057    6]\n",
      "  [   5   39]]\n",
      "\n",
      " [[5055    9]\n",
      "  [   5   38]]\n",
      "\n",
      " [[5063    3]\n",
      "  [   1   40]]\n",
      "\n",
      " [[5069    0]\n",
      "  [   1   37]]\n",
      "\n",
      " [[5049    4]\n",
      "  [   2   52]]\n",
      "\n",
      " [[5052    6]\n",
      "  [   4   45]]\n",
      "\n",
      " [[5068    3]\n",
      "  [   1   35]]\n",
      "\n",
      " [[5063    9]\n",
      "  [   4   31]]\n",
      "\n",
      " [[5044    3]\n",
      "  [   7   53]]\n",
      "\n",
      " [[5046    1]\n",
      "  [   5   55]]\n",
      "\n",
      " [[5061   11]\n",
      "  [   3   32]]\n",
      "\n",
      " [[5039   20]\n",
      "  [   6   42]]\n",
      "\n",
      " [[5045    5]\n",
      "  [   8   49]]\n",
      "\n",
      " [[5067    4]\n",
      "  [  11   25]]\n",
      "\n",
      " [[5057   11]\n",
      "  [   6   33]]\n",
      "\n",
      " [[5051   13]\n",
      "  [   7   36]]\n",
      "\n",
      " [[5077    0]\n",
      "  [   1   29]]\n",
      "\n",
      " [[5066    6]\n",
      "  [   1   34]]\n",
      "\n",
      " [[5069    3]\n",
      "  [   3   32]]\n",
      "\n",
      " [[5051    7]\n",
      "  [   6   43]]\n",
      "\n",
      " [[5076   10]\n",
      "  [   6   15]]\n",
      "\n",
      " [[5056    3]\n",
      "  [  12   36]]\n",
      "\n",
      " [[5045    5]\n",
      "  [  12   45]]\n",
      "\n",
      " [[5070    1]\n",
      "  [   2   34]]\n",
      "\n",
      " [[5069    5]\n",
      "  [   9   24]]\n",
      "\n",
      " [[5076    1]\n",
      "  [   3   27]]\n",
      "\n",
      " [[5047    9]\n",
      "  [   3   48]]\n",
      "\n",
      " [[5059    0]\n",
      "  [  11   37]]\n",
      "\n",
      " [[5067    0]\n",
      "  [   7   33]]\n",
      "\n",
      " [[5055    7]\n",
      "  [   2   43]]]\n",
      "Classification Report : \n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                 affenpinscher       0.90      0.84      0.87        31\n",
      "                  afghan_hound       0.96      0.88      0.92        52\n",
      "           african_hunting_dog       0.96      0.98      0.97        45\n",
      "                      airedale       0.91      0.91      0.91        44\n",
      "american_staffordshire_terrier       0.82      0.78      0.80        46\n",
      "                   appenzeller       0.76      0.78      0.77        32\n",
      "            australian_terrier       0.84      0.73      0.78        44\n",
      "                       basenji       0.96      0.90      0.93        49\n",
      "                        basset       0.93      0.91      0.92        46\n",
      "                        beagle       0.85      0.85      0.85        41\n",
      "            bedlington_terrier       0.95      0.98      0.97        58\n",
      "          bernese_mountain_dog       0.90      1.00      0.95        45\n",
      "              blenheim_spaniel       0.91      1.00      0.95        41\n",
      "                    bloodhound       0.93      0.82      0.88        51\n",
      "                      bluetick       0.92      0.94      0.93        47\n",
      "                 border_collie       0.87      0.85      0.86        40\n",
      "                border_terrier       0.97      0.84      0.90        44\n",
      "                        borzoi       0.92      0.85      0.88        52\n",
      "                   boston_bull       0.82      1.00      0.90        54\n",
      "          bouvier_des_flandres       0.85      0.83      0.84        47\n",
      "                         boxer       0.82      0.93      0.88        30\n",
      "             brabancon_griffon       0.89      0.91      0.90        44\n",
      "                        briard       0.82      0.92      0.87        36\n",
      "              brittany_spaniel       0.93      0.85      0.89        47\n",
      "                  bull_mastiff       0.83      0.95      0.89        42\n",
      "                         cairn       0.88      0.91      0.89        46\n",
      "                      cardigan       0.74      0.95      0.83        44\n",
      "      chesapeake_bay_retriever       0.90      0.92      0.91        51\n",
      "                     chihuahua       0.78      0.86      0.82        42\n",
      "                          chow       0.98      0.95      0.97        44\n",
      "                       clumber       0.97      0.95      0.96        40\n",
      "              coated_retriever       0.92      0.86      0.89        42\n",
      "        coated_wheaten_terrier       0.79      0.91      0.85        33\n",
      "                cocker_spaniel       0.85      0.83      0.84        48\n",
      "                        collie       0.86      0.56      0.68        32\n",
      "                dandie_dinmont       0.96      0.96      0.96        54\n",
      "                         dhole       0.97      0.91      0.94        33\n",
      "                         dingo       0.90      0.85      0.88        33\n",
      "                      doberman       0.94      0.94      0.94        32\n",
      "              english_foxhound       0.72      0.79      0.76        39\n",
      "                english_setter       0.91      0.83      0.87        36\n",
      "              english_springer       0.97      0.95      0.96        38\n",
      "                   entlebucher       0.93      0.89      0.91        45\n",
      "                    eskimo_dog       0.84      0.67      0.74        39\n",
      "                french_bulldog       0.93      0.84      0.89        32\n",
      "               german_shepherd       0.85      0.85      0.85        41\n",
      "               giant_schnauzer       0.89      0.91      0.90        44\n",
      "              golden_retriever       0.86      0.91      0.89        34\n",
      "                 gordon_setter       0.97      0.97      0.97        40\n",
      "                    great_dane       0.83      0.93      0.88        46\n",
      "                great_pyrenees       0.92      0.87      0.90        54\n",
      "    greater_swiss_mountain_dog       0.95      0.87      0.91        47\n",
      "                   groenendael       0.93      0.95      0.94        40\n",
      "            haired_fox_terrier       0.83      0.87      0.85        39\n",
      "                haired_pointer       0.91      0.94      0.93        34\n",
      "                  ibizan_hound       0.88      0.90      0.89        51\n",
      "                  irish_setter       0.97      0.97      0.97        39\n",
      "                 irish_terrier       0.91      0.91      0.91        45\n",
      "           irish_water_spaniel       0.94      0.97      0.95        32\n",
      "               irish_wolfhound       0.88      0.91      0.89        55\n",
      "             italian_greyhound       0.84      0.88      0.86        42\n",
      "              japanese_spaniel       0.98      0.93      0.95        56\n",
      "                      keeshond       0.92      1.00      0.96        35\n",
      "                        kelpie       0.90      0.85      0.88        33\n",
      "            kerry_blue_terrier       0.90      0.98      0.94        45\n",
      "                      komondor       0.97      0.94      0.96        35\n",
      "                        kuvasz       0.87      0.90      0.89        30\n",
      "            labrador_retriever       0.83      0.89      0.86        44\n",
      "              lakeland_terrier       0.87      0.80      0.83        49\n",
      "                      leonberg       0.98      0.97      0.98        61\n",
      "                         lhasa       0.74      0.76      0.75        42\n",
      "                      malamute       0.89      0.75      0.81        52\n",
      "                      malinois       0.85      0.90      0.88        39\n",
      "                   maltese_dog       0.96      0.94      0.95        72\n",
      "              mexican_hairless       0.93      0.93      0.93        29\n",
      "            miniature_pinscher       0.86      0.95      0.90        39\n",
      "              miniature_poodle       0.74      0.67      0.70        39\n",
      "           miniature_schnauzer       0.82      0.84      0.83        37\n",
      "                  newfoundland       0.94      0.86      0.90        36\n",
      "               norfolk_terrier       0.78      0.95      0.86        42\n",
      "            norwegian_elkhound       0.93      0.98      0.95        43\n",
      "               norwich_terrier       0.91      0.83      0.87        47\n",
      "          old_english_sheepdog       0.96      0.94      0.95        47\n",
      "                    otterhound       1.00      0.89      0.94        44\n",
      "                      papillon       0.98      0.94      0.96        48\n",
      "                      pekinese       0.86      0.98      0.92        45\n",
      "                      pembroke       0.97      0.76      0.85        50\n",
      "                    pomeranian       0.98      0.98      0.98        50\n",
      "                           pug       0.91      0.90      0.91        48\n",
      "                       redbone       0.87      0.89      0.88        44\n",
      "           rhodesian_ridgeback       0.81      0.88      0.84        43\n",
      "                    rottweiler       0.93      0.98      0.95        41\n",
      "                 saint_bernard       1.00      0.97      0.99        38\n",
      "                        saluki       0.93      0.96      0.95        54\n",
      "                       samoyed       0.88      0.92      0.90        49\n",
      "                    schipperke       0.92      0.97      0.95        36\n",
      "                scotch_terrier       0.78      0.89      0.83        35\n",
      "            scottish_deerhound       0.95      0.88      0.91        60\n",
      "              sealyham_terrier       0.98      0.92      0.95        60\n",
      "             shetland_sheepdog       0.74      0.91      0.82        35\n",
      "                siberian_husky       0.68      0.88      0.76        48\n",
      "                 silky_terrier       0.91      0.86      0.88        57\n",
      "     staffordshire_bullterrier       0.86      0.69      0.77        36\n",
      "               standard_poodle       0.75      0.85      0.80        39\n",
      "            standard_schnauzer       0.73      0.84      0.78        43\n",
      "                sussex_spaniel       1.00      0.97      0.98        30\n",
      "                 tan_coonhound       0.85      0.97      0.91        35\n",
      "               tibetan_mastiff       0.91      0.91      0.91        35\n",
      "               tibetan_terrier       0.86      0.88      0.87        49\n",
      "                    toy_poodle       0.60      0.71      0.65        21\n",
      "                   toy_terrier       0.92      0.75      0.83        48\n",
      "                           tzu       0.90      0.79      0.84        57\n",
      "                        vizsla       0.97      0.94      0.96        36\n",
      "                  walker_hound       0.83      0.73      0.77        33\n",
      "                    weimaraner       0.96      0.90      0.93        30\n",
      "        welsh_springer_spaniel       0.84      0.94      0.89        51\n",
      "   west_highland_white_terrier       1.00      0.77      0.87        48\n",
      "                       whippet       1.00      0.82      0.90        40\n",
      "             yorkshire_terrier       0.86      0.96      0.91        45\n",
      "\n",
      "                      accuracy                           0.89      5107\n",
      "                     macro avg       0.89      0.89      0.89      5107\n",
      "                  weighted avg       0.89      0.89      0.89      5107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = df.drop(columns=['Breed'])\n",
    "y = df['Breed']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25)\n",
    "\n",
    "# Encode the categorical target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert the encoded labels to one-hot encoding\n",
    "y_train = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n",
    "optimal = create_model(first_layer=128, activation_1='relu', second_layer=64, activation_2='relu', activation_out='softmax', learn_rate=0.0005)\n",
    "optimal.fit(x_train, y_train, epochs=EPOCHS, batch_size=50, validation_data=(x_test, y_test))\n",
    "\n",
    "# Make predictions\n",
    "y_pred_opt = model.predict(x_test)\n",
    "\n",
    "# Convert continuous predictions to class labels\n",
    "y_pred_classes = np.argmax(y_pred_opt, axis=1)\n",
    "\n",
    "# Convert one-hot encoded y_test back to class labels\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Inverse transform the predicted and ground truth class labels to original breed names\n",
    "y_pred_breed = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_test_breed = label_encoder.inverse_transform(y_test_classes)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "multi_conf_matrix = multilabel_confusion_matrix(y_test_breed, y_pred_breed)\n",
    "print(\"Confusion Matrix for each label : \")\n",
    "print(multi_conf_matrix)\n",
    "\n",
    "print(\"Classification Report : \")\n",
    "print(classification_report(y_test_breed, y_pred_breed))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d78e7-22ed-4931-a653-c07ae01c6117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
