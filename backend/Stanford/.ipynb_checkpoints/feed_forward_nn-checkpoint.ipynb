{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c480d74a-d567-46ac-812c-a790df98caa7",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f7599a31-d302-46c6-a558-35b7d3a2ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a3839974-e82f-43f6-9445-7da6e263123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikeras in c:\\users\\kirin\\miniconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: keras in c:\\users\\kirin\\miniconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: packaging>=0.21 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikeras) (23.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikeras) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem<0.32,>=0.23.1 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikeras) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kirin\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikeras keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dfd374a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "84336ead-25d3-401b-9e0e-311c35f29391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "TEST_SIZE = 0.4\n",
    "LEARN_RATE = 0.001\n",
    "FIRST_LAYER = 128 \n",
    "ACTIVATION_1 = 'relu'\n",
    "SECOND_LAYER = 64\n",
    "ACTIVATION_2 = 'relu'\n",
    "ACTIVATION_OUT = 'softmax'\n",
    "LOSS_TYPE = 'categorical_crossentropy'\n",
    "METRICS = ['accuracy']\n",
    "EPOCHS = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7c12fe28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Breed</th>\n",
       "      <th>Feature 0</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature 90</th>\n",
       "      <th>Feature 91</th>\n",
       "      <th>Feature 92</th>\n",
       "      <th>Feature 93</th>\n",
       "      <th>Feature 94</th>\n",
       "      <th>Feature 95</th>\n",
       "      <th>Feature 96</th>\n",
       "      <th>Feature 97</th>\n",
       "      <th>Feature 98</th>\n",
       "      <th>Feature 99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brittany_spaniel</td>\n",
       "      <td>-5.279870</td>\n",
       "      <td>-2.208426</td>\n",
       "      <td>5.211814</td>\n",
       "      <td>-8.099501</td>\n",
       "      <td>-13.185220</td>\n",
       "      <td>-0.753768</td>\n",
       "      <td>-3.015471</td>\n",
       "      <td>0.825370</td>\n",
       "      <td>-4.269372</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.465695</td>\n",
       "      <td>-0.674558</td>\n",
       "      <td>0.123558</td>\n",
       "      <td>-1.240681</td>\n",
       "      <td>-1.069499</td>\n",
       "      <td>-0.714136</td>\n",
       "      <td>1.011624</td>\n",
       "      <td>-0.225715</td>\n",
       "      <td>-0.220330</td>\n",
       "      <td>1.140998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brittany_spaniel</td>\n",
       "      <td>0.524020</td>\n",
       "      <td>-4.321692</td>\n",
       "      <td>-5.808849</td>\n",
       "      <td>-0.097443</td>\n",
       "      <td>-13.157566</td>\n",
       "      <td>0.439865</td>\n",
       "      <td>-4.521323</td>\n",
       "      <td>-1.382092</td>\n",
       "      <td>-1.304059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.410429</td>\n",
       "      <td>0.396908</td>\n",
       "      <td>0.344853</td>\n",
       "      <td>0.243527</td>\n",
       "      <td>-0.416206</td>\n",
       "      <td>0.310021</td>\n",
       "      <td>0.025371</td>\n",
       "      <td>-0.221647</td>\n",
       "      <td>-0.306602</td>\n",
       "      <td>-0.855617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brittany_spaniel</td>\n",
       "      <td>4.484349</td>\n",
       "      <td>-11.409184</td>\n",
       "      <td>-2.061785</td>\n",
       "      <td>-8.795961</td>\n",
       "      <td>-10.736951</td>\n",
       "      <td>-0.650287</td>\n",
       "      <td>0.697348</td>\n",
       "      <td>8.464226</td>\n",
       "      <td>-2.961214</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.486026</td>\n",
       "      <td>0.636264</td>\n",
       "      <td>-0.315544</td>\n",
       "      <td>1.400241</td>\n",
       "      <td>-0.116869</td>\n",
       "      <td>-0.253275</td>\n",
       "      <td>0.918089</td>\n",
       "      <td>-0.645903</td>\n",
       "      <td>0.545067</td>\n",
       "      <td>-1.274210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brittany_spaniel</td>\n",
       "      <td>8.630311</td>\n",
       "      <td>-9.028896</td>\n",
       "      <td>-4.177602</td>\n",
       "      <td>-3.575223</td>\n",
       "      <td>-7.698362</td>\n",
       "      <td>-5.857273</td>\n",
       "      <td>-3.473359</td>\n",
       "      <td>6.915802</td>\n",
       "      <td>1.972978</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.718268</td>\n",
       "      <td>0.056995</td>\n",
       "      <td>0.152345</td>\n",
       "      <td>0.562226</td>\n",
       "      <td>0.279578</td>\n",
       "      <td>-0.277104</td>\n",
       "      <td>-0.956852</td>\n",
       "      <td>-0.531012</td>\n",
       "      <td>0.249064</td>\n",
       "      <td>-1.877944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brittany_spaniel</td>\n",
       "      <td>-6.459163</td>\n",
       "      <td>-5.178344</td>\n",
       "      <td>3.182314</td>\n",
       "      <td>-6.884826</td>\n",
       "      <td>-2.663270</td>\n",
       "      <td>-0.779802</td>\n",
       "      <td>-0.788943</td>\n",
       "      <td>4.521932</td>\n",
       "      <td>-3.636744</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.964177</td>\n",
       "      <td>0.368517</td>\n",
       "      <td>-0.274313</td>\n",
       "      <td>-0.534803</td>\n",
       "      <td>0.797063</td>\n",
       "      <td>-0.156896</td>\n",
       "      <td>1.027882</td>\n",
       "      <td>1.130965</td>\n",
       "      <td>0.696718</td>\n",
       "      <td>0.098893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20423</th>\n",
       "      <td>basenji</td>\n",
       "      <td>9.544751</td>\n",
       "      <td>-11.946300</td>\n",
       "      <td>6.532518</td>\n",
       "      <td>6.032665</td>\n",
       "      <td>-0.054745</td>\n",
       "      <td>-2.835599</td>\n",
       "      <td>8.757513</td>\n",
       "      <td>2.183208</td>\n",
       "      <td>-3.308503</td>\n",
       "      <td>...</td>\n",
       "      <td>2.823255</td>\n",
       "      <td>1.785358</td>\n",
       "      <td>-0.389777</td>\n",
       "      <td>2.354301</td>\n",
       "      <td>-0.200981</td>\n",
       "      <td>1.370857</td>\n",
       "      <td>0.069524</td>\n",
       "      <td>0.747879</td>\n",
       "      <td>-1.344105</td>\n",
       "      <td>-1.186251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20424</th>\n",
       "      <td>basenji</td>\n",
       "      <td>-14.772338</td>\n",
       "      <td>-8.129835</td>\n",
       "      <td>6.266210</td>\n",
       "      <td>9.255338</td>\n",
       "      <td>-0.755042</td>\n",
       "      <td>-10.281703</td>\n",
       "      <td>-4.626710</td>\n",
       "      <td>3.558872</td>\n",
       "      <td>-6.597734</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.323281</td>\n",
       "      <td>-0.496995</td>\n",
       "      <td>-1.813071</td>\n",
       "      <td>0.129572</td>\n",
       "      <td>-0.680327</td>\n",
       "      <td>0.759279</td>\n",
       "      <td>-0.898638</td>\n",
       "      <td>0.825506</td>\n",
       "      <td>-1.794241</td>\n",
       "      <td>0.535946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20425</th>\n",
       "      <td>basenji</td>\n",
       "      <td>-0.651727</td>\n",
       "      <td>-14.567093</td>\n",
       "      <td>19.406995</td>\n",
       "      <td>0.646407</td>\n",
       "      <td>10.396984</td>\n",
       "      <td>-6.700563</td>\n",
       "      <td>-4.514317</td>\n",
       "      <td>13.566748</td>\n",
       "      <td>-2.614418</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.489337</td>\n",
       "      <td>2.132769</td>\n",
       "      <td>0.234950</td>\n",
       "      <td>-1.367633</td>\n",
       "      <td>-1.451568</td>\n",
       "      <td>0.228860</td>\n",
       "      <td>-0.094061</td>\n",
       "      <td>0.696946</td>\n",
       "      <td>-0.543697</td>\n",
       "      <td>-1.004654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20426</th>\n",
       "      <td>basenji</td>\n",
       "      <td>-1.703676</td>\n",
       "      <td>-2.682114</td>\n",
       "      <td>14.418438</td>\n",
       "      <td>7.169161</td>\n",
       "      <td>6.828974</td>\n",
       "      <td>-6.077620</td>\n",
       "      <td>-2.837851</td>\n",
       "      <td>11.472995</td>\n",
       "      <td>-8.041656</td>\n",
       "      <td>...</td>\n",
       "      <td>1.308288</td>\n",
       "      <td>-0.401849</td>\n",
       "      <td>1.429980</td>\n",
       "      <td>-0.723179</td>\n",
       "      <td>-2.111574</td>\n",
       "      <td>-1.035175</td>\n",
       "      <td>-0.324995</td>\n",
       "      <td>0.698054</td>\n",
       "      <td>0.101706</td>\n",
       "      <td>0.878638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20427</th>\n",
       "      <td>basenji</td>\n",
       "      <td>-1.798135</td>\n",
       "      <td>-4.808407</td>\n",
       "      <td>4.453213</td>\n",
       "      <td>6.871646</td>\n",
       "      <td>2.610319</td>\n",
       "      <td>-11.454654</td>\n",
       "      <td>3.632534</td>\n",
       "      <td>-3.067008</td>\n",
       "      <td>-3.319274</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.445148</td>\n",
       "      <td>0.125831</td>\n",
       "      <td>0.543326</td>\n",
       "      <td>-0.379736</td>\n",
       "      <td>-1.181225</td>\n",
       "      <td>1.674236</td>\n",
       "      <td>0.643497</td>\n",
       "      <td>1.456580</td>\n",
       "      <td>-0.038680</td>\n",
       "      <td>1.196227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20428 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Breed  Feature 0  Feature 1  Feature 2  Feature 3  \\\n",
       "0      brittany_spaniel  -5.279870  -2.208426   5.211814  -8.099501   \n",
       "1      brittany_spaniel   0.524020  -4.321692  -5.808849  -0.097443   \n",
       "2      brittany_spaniel   4.484349 -11.409184  -2.061785  -8.795961   \n",
       "3      brittany_spaniel   8.630311  -9.028896  -4.177602  -3.575223   \n",
       "4      brittany_spaniel  -6.459163  -5.178344   3.182314  -6.884826   \n",
       "...                 ...        ...        ...        ...        ...   \n",
       "20423           basenji   9.544751 -11.946300   6.532518   6.032665   \n",
       "20424           basenji -14.772338  -8.129835   6.266210   9.255338   \n",
       "20425           basenji  -0.651727 -14.567093  19.406995   0.646407   \n",
       "20426           basenji  -1.703676  -2.682114  14.418438   7.169161   \n",
       "20427           basenji  -1.798135  -4.808407   4.453213   6.871646   \n",
       "\n",
       "       Feature 4  Feature 5  Feature 6  Feature 7  Feature 8  ...  Feature 90  \\\n",
       "0     -13.185220  -0.753768  -3.015471   0.825370  -4.269372  ...   -0.465695   \n",
       "1     -13.157566   0.439865  -4.521323  -1.382092  -1.304059  ...   -0.410429   \n",
       "2     -10.736951  -0.650287   0.697348   8.464226  -2.961214  ...   -1.486026   \n",
       "3      -7.698362  -5.857273  -3.473359   6.915802   1.972978  ...   -1.718268   \n",
       "4      -2.663270  -0.779802  -0.788943   4.521932  -3.636744  ...   -0.964177   \n",
       "...          ...        ...        ...        ...        ...  ...         ...   \n",
       "20423  -0.054745  -2.835599   8.757513   2.183208  -3.308503  ...    2.823255   \n",
       "20424  -0.755042 -10.281703  -4.626710   3.558872  -6.597734  ...   -1.323281   \n",
       "20425  10.396984  -6.700563  -4.514317  13.566748  -2.614418  ...   -0.489337   \n",
       "20426   6.828974  -6.077620  -2.837851  11.472995  -8.041656  ...    1.308288   \n",
       "20427   2.610319 -11.454654   3.632534  -3.067008  -3.319274  ...   -1.445148   \n",
       "\n",
       "       Feature 91  Feature 92  Feature 93  Feature 94  Feature 95  Feature 96  \\\n",
       "0       -0.674558    0.123558   -1.240681   -1.069499   -0.714136    1.011624   \n",
       "1        0.396908    0.344853    0.243527   -0.416206    0.310021    0.025371   \n",
       "2        0.636264   -0.315544    1.400241   -0.116869   -0.253275    0.918089   \n",
       "3        0.056995    0.152345    0.562226    0.279578   -0.277104   -0.956852   \n",
       "4        0.368517   -0.274313   -0.534803    0.797063   -0.156896    1.027882   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "20423    1.785358   -0.389777    2.354301   -0.200981    1.370857    0.069524   \n",
       "20424   -0.496995   -1.813071    0.129572   -0.680327    0.759279   -0.898638   \n",
       "20425    2.132769    0.234950   -1.367633   -1.451568    0.228860   -0.094061   \n",
       "20426   -0.401849    1.429980   -0.723179   -2.111574   -1.035175   -0.324995   \n",
       "20427    0.125831    0.543326   -0.379736   -1.181225    1.674236    0.643497   \n",
       "\n",
       "       Feature 97  Feature 98  Feature 99  \n",
       "0       -0.225715   -0.220330    1.140998  \n",
       "1       -0.221647   -0.306602   -0.855617  \n",
       "2       -0.645903    0.545067   -1.274210  \n",
       "3       -0.531012    0.249064   -1.877944  \n",
       "4        1.130965    0.696718    0.098893  \n",
       "...           ...         ...         ...  \n",
       "20423    0.747879   -1.344105   -1.186251  \n",
       "20424    0.825506   -1.794241    0.535946  \n",
       "20425    0.696946   -0.543697   -1.004654  \n",
       "20426    0.698054    0.101706    0.878638  \n",
       "20427    1.456580   -0.038680    1.196227  \n",
       "\n",
       "[20428 rows x 101 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('features.csv')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7f989db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the number of samples to take for each breed\n",
    "# breed_counts = df['Breed'].value_counts()\n",
    "# max_count = breed_counts.max()\n",
    "# sample_sizes = np.maximum(20, 20 + (breed_counts / max_count * 20).astype(int))\n",
    "\n",
    "# # Create an empty list to store sampled dataframes\n",
    "# sampled_dfs = []\n",
    "\n",
    "# # Iterate over each breed to sample data\n",
    "# for breed, sample_size in sample_sizes.items():\n",
    "#     # Get indices of rows corresponding to the current breed\n",
    "#     breed_indices = df[df['Breed'] == breed].index\n",
    "    \n",
    "#     # Randomly sample rows for the current breed\n",
    "#     sampled_indices = np.random.choice(breed_indices, size=sample_size, replace=False)\n",
    "    \n",
    "#     # Append sampled rows to the list\n",
    "#     sampled_dfs.append(df.loc[sampled_indices])\n",
    "\n",
    "# # Concatenate all sampled dataframes into one\n",
    "# sampled_df = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "# display(sample_sizes.head())\n",
    "# display(sampled_df.head())\n",
    "\n",
    "# x = sampled_df.drop(columns=['Breed'])\n",
    "# y = sampled_df['Breed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "20e9f19a-4a2a-496b-b444-21cc53a0b516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "x = df.drop(columns=['Breed'])\n",
    "y = df['Breed']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = TEST_SIZE)\n",
    "num_classes = 120\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be0facdc-6b55-44b7-9e58-cd09912f0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert the encoded labels to one-hot encoding\n",
    "y_train = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test_encoded, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b11822a6-d107-4ba2-b268-e31efe07dc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "383/383 [==============================] - 2s 2ms/step - loss: 2.0280 - accuracy: 0.5148 - val_loss: 0.9252 - val_accuracy: 0.7193\n",
      "Epoch 2/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.7279 - accuracy: 0.7750 - val_loss: 0.7673 - val_accuracy: 0.7608\n",
      "Epoch 3/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.5713 - accuracy: 0.8173 - val_loss: 0.7707 - val_accuracy: 0.7652\n",
      "Epoch 4/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.4870 - accuracy: 0.8423 - val_loss: 0.7404 - val_accuracy: 0.7790\n",
      "Epoch 5/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.4285 - accuracy: 0.8562 - val_loss: 0.7319 - val_accuracy: 0.7832\n",
      "Epoch 6/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.3733 - accuracy: 0.8748 - val_loss: 0.7892 - val_accuracy: 0.7690\n",
      "Epoch 7/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.3331 - accuracy: 0.8881 - val_loss: 0.7750 - val_accuracy: 0.7789\n",
      "Epoch 8/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.2939 - accuracy: 0.9028 - val_loss: 0.7969 - val_accuracy: 0.7761\n",
      "Epoch 9/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.2574 - accuracy: 0.9152 - val_loss: 0.8251 - val_accuracy: 0.7758\n",
      "Epoch 10/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.2298 - accuracy: 0.9222 - val_loss: 0.8601 - val_accuracy: 0.7704\n",
      "Epoch 11/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.2042 - accuracy: 0.9346 - val_loss: 0.8969 - val_accuracy: 0.7717\n",
      "Epoch 12/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.1826 - accuracy: 0.9401 - val_loss: 0.9270 - val_accuracy: 0.7759\n",
      "Epoch 13/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.1656 - accuracy: 0.9469 - val_loss: 0.9554 - val_accuracy: 0.7730\n",
      "Epoch 14/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.1462 - accuracy: 0.9525 - val_loss: 0.9826 - val_accuracy: 0.7736\n",
      "Epoch 15/20\n",
      "383/383 [==============================] - 1s 2ms/step - loss: 0.1339 - accuracy: 0.9574 - val_loss: 1.0507 - val_accuracy: 0.7657\n",
      "Epoch 16/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.9634 - val_loss: 1.0647 - val_accuracy: 0.7720\n",
      "Epoch 17/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.1022 - accuracy: 0.9693 - val_loss: 1.1223 - val_accuracy: 0.7729\n",
      "Epoch 18/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.0957 - accuracy: 0.9693 - val_loss: 1.1520 - val_accuracy: 0.7714\n",
      "Epoch 19/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.0902 - accuracy: 0.9731 - val_loss: 1.2048 - val_accuracy: 0.7709\n",
      "Epoch 20/20\n",
      "383/383 [==============================] - 1s 1ms/step - loss: 0.0734 - accuracy: 0.9783 - val_loss: 1.2244 - val_accuracy: 0.7690\n",
      "256/256 [==============================] - 0s 733us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(FIRST_LAYER, input_shape=(100,), activation=ACTIVATION_1)) # Assuming 100 features\n",
    "model.add(Dense(SECOND_LAYER, activation=ACTIVATION_2))\n",
    "model.add(Dense(num_classes, activation=ACTIVATION_OUT)) # num_classes is the number of unique breed labels\n",
    "model.compile(optimizer=Adam(learning_rate=LEARN_RATE), loss=LOSS_TYPE, metrics=METRICS)\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=EPOCHS, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "80d1b997-4437-4ad5-ba4e-008b127f9067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 0s 745us/step - loss: 1.2244 - accuracy: 0.7690\n",
      "Test Accuracy: 0.7689672112464905\n",
      "Confusion Matrix for each label : \n",
      "[[[8107    6]\n",
      "  [   9   50]]\n",
      "\n",
      " [[8059    5]\n",
      "  [  14   94]]\n",
      "\n",
      " [[8104    7]\n",
      "  [   7   54]]\n",
      "\n",
      " [[8068   19]\n",
      "  [  20   65]]\n",
      "\n",
      " [[8074   30]\n",
      "  [  22   46]]\n",
      "\n",
      " [[8081   26]\n",
      "  [  31   34]]\n",
      "\n",
      " [[8084   16]\n",
      "  [  27   45]]\n",
      "\n",
      " [[8069   12]\n",
      "  [  18   73]]\n",
      "\n",
      " [[8085   13]\n",
      "  [  11   63]]\n",
      "\n",
      " [[8070   27]\n",
      "  [  15   60]]\n",
      "\n",
      " [[8093   12]\n",
      "  [   5   62]]\n",
      "\n",
      " [[8072   15]\n",
      "  [   6   79]]\n",
      "\n",
      " [[8087   10]\n",
      "  [   5   70]]\n",
      "\n",
      " [[8093    8]\n",
      "  [  20   51]]\n",
      "\n",
      " [[8089   12]\n",
      "  [   8   63]]\n",
      "\n",
      " [[8087   31]\n",
      "  [  20   34]]\n",
      "\n",
      " [[8095    8]\n",
      "  [  10   59]]\n",
      "\n",
      " [[8111    7]\n",
      "  [  13   41]]\n",
      "\n",
      " [[8071   27]\n",
      "  [   3   71]]\n",
      "\n",
      " [[8082   25]\n",
      "  [  19   46]]\n",
      "\n",
      " [[8112   15]\n",
      "  [   7   38]]\n",
      "\n",
      " [[8097   14]\n",
      "  [   8   53]]\n",
      "\n",
      " [[8093   22]\n",
      "  [  18   39]]\n",
      "\n",
      " [[8104    8]\n",
      "  [  16   44]]\n",
      "\n",
      " [[8100   13]\n",
      "  [   7   52]]\n",
      "\n",
      " [[8078   13]\n",
      "  [  20   61]]\n",
      "\n",
      " [[8067   38]\n",
      "  [  13   54]]\n",
      "\n",
      " [[8096   11]\n",
      "  [  16   49]]\n",
      "\n",
      " [[8092   23]\n",
      "  [  15   42]]\n",
      "\n",
      " [[8082    3]\n",
      "  [   5   82]]\n",
      "\n",
      " [[8111    4]\n",
      "  [   2   55]]\n",
      "\n",
      " [[8094    9]\n",
      "  [  18   51]]\n",
      "\n",
      " [[8102   23]\n",
      "  [   9   38]]\n",
      "\n",
      " [[8086   14]\n",
      "  [  22   50]]\n",
      "\n",
      " [[8085   15]\n",
      "  [  54   18]]\n",
      "\n",
      " [[8100    5]\n",
      "  [   9   58]]\n",
      "\n",
      " [[8102    5]\n",
      "  [   8   57]]\n",
      "\n",
      " [[8098   14]\n",
      "  [  22   38]]\n",
      "\n",
      " [[8109   11]\n",
      "  [   9   43]]\n",
      "\n",
      " [[8086   23]\n",
      "  [  29   34]]\n",
      "\n",
      " [[8095    4]\n",
      "  [  25   48]]\n",
      "\n",
      " [[8102    7]\n",
      "  [  14   49]]\n",
      "\n",
      " [[8088   16]\n",
      "  [  20   48]]\n",
      "\n",
      " [[8082   24]\n",
      "  [  51   15]]\n",
      "\n",
      " [[8106    8]\n",
      "  [  17   41]]\n",
      "\n",
      " [[8096   12]\n",
      "  [  17   47]]\n",
      "\n",
      " [[8091   22]\n",
      "  [  21   38]]\n",
      "\n",
      " [[8089   18]\n",
      "  [  11   54]]\n",
      "\n",
      " [[8106    6]\n",
      "  [   4   56]]\n",
      "\n",
      " [[8097   24]\n",
      "  [   8   43]]\n",
      "\n",
      " [[8075   16]\n",
      "  [  20   61]]\n",
      "\n",
      " [[8099   12]\n",
      "  [  11   50]]\n",
      "\n",
      " [[8100    8]\n",
      "  [   7   57]]\n",
      "\n",
      " [[8096   19]\n",
      "  [  17   40]]\n",
      "\n",
      " [[8116    8]\n",
      "  [   5   43]]\n",
      "\n",
      " [[8071   20]\n",
      "  [   8   73]]\n",
      "\n",
      " [[8090   10]\n",
      "  [   8   64]]\n",
      "\n",
      " [[8088   10]\n",
      "  [  18   56]]\n",
      "\n",
      " [[8101    9]\n",
      "  [   4   58]]\n",
      "\n",
      " [[8060   21]\n",
      "  [  25   66]]\n",
      "\n",
      " [[8075   21]\n",
      "  [  15   61]]\n",
      "\n",
      " [[8088    2]\n",
      "  [  12   70]]\n",
      "\n",
      " [[8114    5]\n",
      "  [   1   52]]\n",
      "\n",
      " [[8108    7]\n",
      "  [  24   33]]\n",
      "\n",
      " [[8090   13]\n",
      "  [   8   61]]\n",
      "\n",
      " [[8095    7]\n",
      "  [   4   66]]\n",
      "\n",
      " [[8102    9]\n",
      "  [  14   47]]\n",
      "\n",
      " [[8074   33]\n",
      "  [  19   46]]\n",
      "\n",
      " [[8057   26]\n",
      "  [  32   57]]\n",
      "\n",
      " [[8066    8]\n",
      "  [  11   87]]\n",
      "\n",
      " [[8054   37]\n",
      "  [  39   42]]\n",
      "\n",
      " [[8064   27]\n",
      "  [  28   53]]\n",
      "\n",
      " [[8086   15]\n",
      "  [  14   57]]\n",
      "\n",
      " [[8063   12]\n",
      "  [  13   84]]\n",
      "\n",
      " [[8093    6]\n",
      "  [   9   64]]\n",
      "\n",
      " [[8081   18]\n",
      "  [  14   59]]\n",
      "\n",
      " [[8082   26]\n",
      "  [  41   23]]\n",
      "\n",
      " [[8093   20]\n",
      "  [  15   44]]\n",
      "\n",
      " [[8075   19]\n",
      "  [  26   52]]\n",
      "\n",
      " [[8064   39]\n",
      "  [  14   55]]\n",
      "\n",
      " [[8078   15]\n",
      "  [   3   76]]\n",
      "\n",
      " [[8094   15]\n",
      "  [  23   40]]\n",
      "\n",
      " [[8093    7]\n",
      "  [  10   62]]\n",
      "\n",
      " [[8112    1]\n",
      "  [  18   41]]\n",
      "\n",
      " [[8093    4]\n",
      "  [   9   66]]\n",
      "\n",
      " [[8084   21]\n",
      "  [   5   62]]\n",
      "\n",
      " [[8091   10]\n",
      "  [  29   42]]\n",
      "\n",
      " [[8084    5]\n",
      "  [   9   74]]\n",
      "\n",
      " [[8081   12]\n",
      "  [  11   68]]\n",
      "\n",
      " [[8092   14]\n",
      "  [  22   44]]\n",
      "\n",
      " [[8082   29]\n",
      "  [  21   40]]\n",
      "\n",
      " [[8106   14]\n",
      "  [   2   50]]\n",
      "\n",
      " [[8102    3]\n",
      "  [   7   60]]\n",
      "\n",
      " [[8064   21]\n",
      "  [   6   81]]\n",
      "\n",
      " [[8049   16]\n",
      "  [   8   99]]\n",
      "\n",
      " [[8098   15]\n",
      "  [   6   53]]\n",
      "\n",
      " [[8084   18]\n",
      "  [  14   56]]\n",
      "\n",
      " [[8065   15]\n",
      "  [  23   69]]\n",
      "\n",
      " [[8079    6]\n",
      "  [  12   75]]\n",
      "\n",
      " [[8072   36]\n",
      "  [  11   53]]\n",
      "\n",
      " [[8051   47]\n",
      "  [  28   46]]\n",
      "\n",
      " [[8082   26]\n",
      "  [  24   40]]\n",
      "\n",
      " [[8096   16]\n",
      "  [  33   27]]\n",
      "\n",
      " [[8072   38]\n",
      "  [  17   45]]\n",
      "\n",
      " [[8082   31]\n",
      "  [  20   39]]\n",
      "\n",
      " [[8118    5]\n",
      "  [   3   46]]\n",
      "\n",
      " [[8094   15]\n",
      "  [  12   51]]\n",
      "\n",
      " [[8107   15]\n",
      "  [  11   39]]\n",
      "\n",
      " [[8070   25]\n",
      "  [  24   53]]\n",
      "\n",
      " [[8106   19]\n",
      "  [  23   24]]\n",
      "\n",
      " [[8081   11]\n",
      "  [  26   54]]\n",
      "\n",
      " [[8046   29]\n",
      "  [  30   67]]\n",
      "\n",
      " [[8104   13]\n",
      "  [   8   47]]\n",
      "\n",
      " [[8090   22]\n",
      "  [  27   33]]\n",
      "\n",
      " [[8100    4]\n",
      "  [  13   55]]\n",
      "\n",
      " [[8086   34]\n",
      "  [   9   43]]\n",
      "\n",
      " [[8098    3]\n",
      "  [  25   46]]\n",
      "\n",
      " [[8089    7]\n",
      "  [  36   40]]\n",
      "\n",
      " [[8086   28]\n",
      "  [  16   42]]]\n",
      "Classification Report : \n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                 affenpinscher       0.89      0.85      0.87        59\n",
      "                  afghan_hound       0.95      0.87      0.91       108\n",
      "           african_hunting_dog       0.89      0.89      0.89        61\n",
      "                      airedale       0.77      0.76      0.77        85\n",
      "american_staffordshire_terrier       0.61      0.68      0.64        68\n",
      "                   appenzeller       0.57      0.52      0.54        65\n",
      "            australian_terrier       0.74      0.62      0.68        72\n",
      "                       basenji       0.86      0.80      0.83        91\n",
      "                        basset       0.83      0.85      0.84        74\n",
      "                        beagle       0.69      0.80      0.74        75\n",
      "            bedlington_terrier       0.84      0.93      0.88        67\n",
      "          bernese_mountain_dog       0.84      0.93      0.88        85\n",
      "              blenheim_spaniel       0.88      0.93      0.90        75\n",
      "                    bloodhound       0.86      0.72      0.78        71\n",
      "                      bluetick       0.84      0.89      0.86        71\n",
      "                 border_collie       0.52      0.63      0.57        54\n",
      "                border_terrier       0.88      0.86      0.87        69\n",
      "                        borzoi       0.85      0.76      0.80        54\n",
      "                   boston_bull       0.72      0.96      0.83        74\n",
      "          bouvier_des_flandres       0.65      0.71      0.68        65\n",
      "                         boxer       0.72      0.84      0.78        45\n",
      "             brabancon_griffon       0.79      0.87      0.83        61\n",
      "                        briard       0.64      0.68      0.66        57\n",
      "              brittany_spaniel       0.85      0.73      0.79        60\n",
      "                  bull_mastiff       0.80      0.88      0.84        59\n",
      "                         cairn       0.82      0.75      0.79        81\n",
      "                      cardigan       0.59      0.81      0.68        67\n",
      "      chesapeake_bay_retriever       0.82      0.75      0.78        65\n",
      "                     chihuahua       0.65      0.74      0.69        57\n",
      "                          chow       0.96      0.94      0.95        87\n",
      "                       clumber       0.93      0.96      0.95        57\n",
      "              coated_retriever       0.85      0.74      0.79        69\n",
      "        coated_wheaten_terrier       0.62      0.81      0.70        47\n",
      "                cocker_spaniel       0.78      0.69      0.74        72\n",
      "                        collie       0.55      0.25      0.34        72\n",
      "                dandie_dinmont       0.92      0.87      0.89        67\n",
      "                         dhole       0.92      0.88      0.90        65\n",
      "                         dingo       0.73      0.63      0.68        60\n",
      "                      doberman       0.80      0.83      0.81        52\n",
      "              english_foxhound       0.60      0.54      0.57        63\n",
      "                english_setter       0.92      0.66      0.77        73\n",
      "              english_springer       0.88      0.78      0.82        63\n",
      "                   entlebucher       0.75      0.71      0.73        68\n",
      "                    eskimo_dog       0.38      0.23      0.29        66\n",
      "                french_bulldog       0.84      0.71      0.77        58\n",
      "               german_shepherd       0.80      0.73      0.76        64\n",
      "               giant_schnauzer       0.63      0.64      0.64        59\n",
      "              golden_retriever       0.75      0.83      0.79        65\n",
      "                 gordon_setter       0.90      0.93      0.92        60\n",
      "                    great_dane       0.64      0.84      0.73        51\n",
      "                great_pyrenees       0.79      0.75      0.77        81\n",
      "    greater_swiss_mountain_dog       0.81      0.82      0.81        61\n",
      "                   groenendael       0.88      0.89      0.88        64\n",
      "            haired_fox_terrier       0.68      0.70      0.69        57\n",
      "                haired_pointer       0.84      0.90      0.87        48\n",
      "                  ibizan_hound       0.78      0.90      0.84        81\n",
      "                  irish_setter       0.86      0.89      0.88        72\n",
      "                 irish_terrier       0.85      0.76      0.80        74\n",
      "           irish_water_spaniel       0.87      0.94      0.90        62\n",
      "               irish_wolfhound       0.76      0.73      0.74        91\n",
      "             italian_greyhound       0.74      0.80      0.77        76\n",
      "              japanese_spaniel       0.97      0.85      0.91        82\n",
      "                      keeshond       0.91      0.98      0.95        53\n",
      "                        kelpie       0.82      0.58      0.68        57\n",
      "            kerry_blue_terrier       0.82      0.88      0.85        69\n",
      "                      komondor       0.90      0.94      0.92        70\n",
      "                        kuvasz       0.84      0.77      0.80        61\n",
      "            labrador_retriever       0.58      0.71      0.64        65\n",
      "              lakeland_terrier       0.69      0.64      0.66        89\n",
      "                      leonberg       0.92      0.89      0.90        98\n",
      "                         lhasa       0.53      0.52      0.53        81\n",
      "                      malamute       0.66      0.65      0.66        81\n",
      "                      malinois       0.79      0.80      0.80        71\n",
      "                   maltese_dog       0.88      0.87      0.87        97\n",
      "              mexican_hairless       0.91      0.88      0.90        73\n",
      "            miniature_pinscher       0.77      0.81      0.79        73\n",
      "              miniature_poodle       0.47      0.36      0.41        64\n",
      "           miniature_schnauzer       0.69      0.75      0.72        59\n",
      "                  newfoundland       0.73      0.67      0.70        78\n",
      "               norfolk_terrier       0.59      0.80      0.67        69\n",
      "            norwegian_elkhound       0.84      0.96      0.89        79\n",
      "               norwich_terrier       0.73      0.63      0.68        63\n",
      "          old_english_sheepdog       0.90      0.86      0.88        72\n",
      "                    otterhound       0.98      0.69      0.81        59\n",
      "                      papillon       0.94      0.88      0.91        75\n",
      "                      pekinese       0.75      0.93      0.83        67\n",
      "                      pembroke       0.81      0.59      0.68        71\n",
      "                    pomeranian       0.94      0.89      0.91        83\n",
      "                           pug       0.85      0.86      0.86        79\n",
      "                       redbone       0.76      0.67      0.71        66\n",
      "           rhodesian_ridgeback       0.58      0.66      0.62        61\n",
      "                    rottweiler       0.78      0.96      0.86        52\n",
      "                 saint_bernard       0.95      0.90      0.92        67\n",
      "                        saluki       0.79      0.93      0.86        87\n",
      "                       samoyed       0.86      0.93      0.89       107\n",
      "                    schipperke       0.78      0.90      0.83        59\n",
      "                scotch_terrier       0.76      0.80      0.78        70\n",
      "            scottish_deerhound       0.82      0.75      0.78        92\n",
      "              sealyham_terrier       0.93      0.86      0.89        87\n",
      "             shetland_sheepdog       0.60      0.83      0.69        64\n",
      "                siberian_husky       0.49      0.62      0.55        74\n",
      "                 silky_terrier       0.61      0.62      0.62        64\n",
      "     staffordshire_bullterrier       0.63      0.45      0.52        60\n",
      "               standard_poodle       0.54      0.73      0.62        62\n",
      "            standard_schnauzer       0.56      0.66      0.60        59\n",
      "                sussex_spaniel       0.90      0.94      0.92        49\n",
      "                 tan_coonhound       0.77      0.81      0.79        63\n",
      "               tibetan_mastiff       0.72      0.78      0.75        50\n",
      "               tibetan_terrier       0.68      0.69      0.68        77\n",
      "                    toy_poodle       0.56      0.51      0.53        47\n",
      "                   toy_terrier       0.83      0.68      0.74        80\n",
      "                           tzu       0.70      0.69      0.69        97\n",
      "                        vizsla       0.78      0.85      0.82        55\n",
      "                  walker_hound       0.60      0.55      0.57        60\n",
      "                    weimaraner       0.93      0.81      0.87        68\n",
      "        welsh_springer_spaniel       0.56      0.83      0.67        52\n",
      "   west_highland_white_terrier       0.94      0.65      0.77        71\n",
      "                       whippet       0.85      0.53      0.65        76\n",
      "             yorkshire_terrier       0.60      0.72      0.66        58\n",
      "\n",
      "                      accuracy                           0.77      8172\n",
      "                     macro avg       0.77      0.77      0.76      8172\n",
      "                  weighted avg       0.77      0.77      0.77      8172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Convert continuous predictions to class labels\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert one-hot encoded y_test back to class labels\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Inverse transform the predicted and ground truth class labels to original breed names\n",
    "y_pred_breed = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_test_breed = label_encoder.inverse_transform(y_test_classes)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "#print(\"Accuracy : \", accuracy_score(y_test, y_pred))\n",
    "#print(\"Mean Square Error : \", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix for each label : \")\n",
    "print(multilabel_confusion_matrix(y_test_breed, y_pred_breed))\n",
    "\n",
    "print(\"Classification Report : \")\n",
    "print(classification_report(y_test_breed, y_pred_breed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cbbb7-a7ae-4562-a23a-440508f93157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
